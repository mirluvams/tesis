{
  "best_metric": 0.8473700212830647,
  "best_model_checkpoint": "MimModels/swinv2-base-patch4-window16-256-popocatepetl-reclassified/checkpoint-4662",
  "epoch": 21.0,
  "eval_steps": 500,
  "global_step": 10878,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.04826254826254826,
      "grad_norm": 3.79866361618042,
      "learning_rate": 9.266409266409266e-07,
      "loss": 1.5626,
      "step": 25
    },
    {
      "epoch": 0.09652509652509653,
      "grad_norm": 4.188299655914307,
      "learning_rate": 1.891891891891892e-06,
      "loss": 1.4997,
      "step": 50
    },
    {
      "epoch": 0.14478764478764478,
      "grad_norm": 3.106621026992798,
      "learning_rate": 2.8571428571428573e-06,
      "loss": 1.4028,
      "step": 75
    },
    {
      "epoch": 0.19305019305019305,
      "grad_norm": 4.14212703704834,
      "learning_rate": 3.822393822393822e-06,
      "loss": 1.2209,
      "step": 100
    },
    {
      "epoch": 0.2413127413127413,
      "grad_norm": 5.635255336761475,
      "learning_rate": 4.749034749034749e-06,
      "loss": 1.0915,
      "step": 125
    },
    {
      "epoch": 0.28957528957528955,
      "grad_norm": 4.575883865356445,
      "learning_rate": 5.7142857142857145e-06,
      "loss": 1.0001,
      "step": 150
    },
    {
      "epoch": 0.33783783783783783,
      "grad_norm": 3.9603912830352783,
      "learning_rate": 6.67953667953668e-06,
      "loss": 0.962,
      "step": 175
    },
    {
      "epoch": 0.3861003861003861,
      "grad_norm": 6.366428852081299,
      "learning_rate": 7.606177606177607e-06,
      "loss": 0.9752,
      "step": 200
    },
    {
      "epoch": 0.4343629343629344,
      "grad_norm": 5.086375713348389,
      "learning_rate": 8.571428571428573e-06,
      "loss": 0.8769,
      "step": 225
    },
    {
      "epoch": 0.4826254826254826,
      "grad_norm": 6.342814922332764,
      "learning_rate": 9.536679536679537e-06,
      "loss": 0.8029,
      "step": 250
    },
    {
      "epoch": 0.5308880308880309,
      "grad_norm": 6.492892742156982,
      "learning_rate": 1.0501930501930504e-05,
      "loss": 0.7534,
      "step": 275
    },
    {
      "epoch": 0.5791505791505791,
      "grad_norm": 6.572014808654785,
      "learning_rate": 1.1467181467181468e-05,
      "loss": 0.7631,
      "step": 300
    },
    {
      "epoch": 0.6274131274131274,
      "grad_norm": Infinity,
      "learning_rate": 1.2393822393822395e-05,
      "loss": 0.7083,
      "step": 325
    },
    {
      "epoch": 0.6756756756756757,
      "grad_norm": 8.740297317504883,
      "learning_rate": 1.335907335907336e-05,
      "loss": 0.7321,
      "step": 350
    },
    {
      "epoch": 0.7239382239382239,
      "grad_norm": 6.0717573165893555,
      "learning_rate": 1.4324324324324326e-05,
      "loss": 0.7042,
      "step": 375
    },
    {
      "epoch": 0.7722007722007722,
      "grad_norm": 9.468854904174805,
      "learning_rate": 1.5289575289575288e-05,
      "loss": 0.7228,
      "step": 400
    },
    {
      "epoch": 0.8204633204633205,
      "grad_norm": 7.39054536819458,
      "learning_rate": 1.6254826254826254e-05,
      "loss": 0.7237,
      "step": 425
    },
    {
      "epoch": 0.8687258687258688,
      "grad_norm": 7.457557201385498,
      "learning_rate": 1.722007722007722e-05,
      "loss": 0.6922,
      "step": 450
    },
    {
      "epoch": 0.916988416988417,
      "grad_norm": 9.222833633422852,
      "learning_rate": 1.8185328185328183e-05,
      "loss": 0.7222,
      "step": 475
    },
    {
      "epoch": 0.9652509652509652,
      "grad_norm": 7.000171184539795,
      "learning_rate": 1.915057915057915e-05,
      "loss": 0.6891,
      "step": 500
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.7892976588628763,
      "eval_f1_weighted": 0.7753802638904096,
      "eval_loss": 0.5468042492866516,
      "eval_runtime": 41.3781,
      "eval_samples_per_second": 79.486,
      "eval_steps_per_second": 0.846,
      "step": 518
    },
    {
      "epoch": 1.0135135135135136,
      "grad_norm": 8.067571640014648,
      "learning_rate": 2.0115830115830116e-05,
      "loss": 0.6677,
      "step": 525
    },
    {
      "epoch": 1.0617760617760619,
      "grad_norm": 9.68215274810791,
      "learning_rate": 2.1081081081081082e-05,
      "loss": 0.6382,
      "step": 550
    },
    {
      "epoch": 1.1100386100386102,
      "grad_norm": 7.304201602935791,
      "learning_rate": 2.2046332046332045e-05,
      "loss": 0.6603,
      "step": 575
    },
    {
      "epoch": 1.1583011583011582,
      "grad_norm": 5.8923821449279785,
      "learning_rate": 2.301158301158301e-05,
      "loss": 0.6518,
      "step": 600
    },
    {
      "epoch": 1.2065637065637065,
      "grad_norm": 6.2222747802734375,
      "learning_rate": 2.3976833976833978e-05,
      "loss": 0.6686,
      "step": 625
    },
    {
      "epoch": 1.2548262548262548,
      "grad_norm": 5.256279468536377,
      "learning_rate": 2.494208494208494e-05,
      "loss": 0.5941,
      "step": 650
    },
    {
      "epoch": 1.303088803088803,
      "grad_norm": 5.800609111785889,
      "learning_rate": 2.590733590733591e-05,
      "loss": 0.6237,
      "step": 675
    },
    {
      "epoch": 1.3513513513513513,
      "grad_norm": 6.6509552001953125,
      "learning_rate": 2.6872586872586873e-05,
      "loss": 0.652,
      "step": 700
    },
    {
      "epoch": 1.3996138996138996,
      "grad_norm": 6.923337936401367,
      "learning_rate": 2.7837837837837836e-05,
      "loss": 0.636,
      "step": 725
    },
    {
      "epoch": 1.4478764478764479,
      "grad_norm": 6.626359462738037,
      "learning_rate": 2.8803088803088806e-05,
      "loss": 0.6551,
      "step": 750
    },
    {
      "epoch": 1.4961389961389961,
      "grad_norm": 5.4526686668396,
      "learning_rate": 2.976833976833977e-05,
      "loss": 0.6468,
      "step": 775
    },
    {
      "epoch": 1.5444015444015444,
      "grad_norm": 7.212571620941162,
      "learning_rate": 3.073359073359073e-05,
      "loss": 0.5889,
      "step": 800
    },
    {
      "epoch": 1.5926640926640927,
      "grad_norm": 5.215142726898193,
      "learning_rate": 3.16988416988417e-05,
      "loss": 0.5546,
      "step": 825
    },
    {
      "epoch": 1.640926640926641,
      "grad_norm": 6.775128364562988,
      "learning_rate": 3.2664092664092665e-05,
      "loss": 0.6067,
      "step": 850
    },
    {
      "epoch": 1.689189189189189,
      "grad_norm": 6.172781944274902,
      "learning_rate": 3.362934362934363e-05,
      "loss": 0.5973,
      "step": 875
    },
    {
      "epoch": 1.7374517374517375,
      "grad_norm": 5.9123640060424805,
      "learning_rate": 3.45945945945946e-05,
      "loss": 0.5709,
      "step": 900
    },
    {
      "epoch": 1.7857142857142856,
      "grad_norm": 10.027604103088379,
      "learning_rate": 3.555984555984556e-05,
      "loss": 0.5937,
      "step": 925
    },
    {
      "epoch": 1.833976833976834,
      "grad_norm": 4.79209041595459,
      "learning_rate": 3.652509652509652e-05,
      "loss": 0.6458,
      "step": 950
    },
    {
      "epoch": 1.8822393822393821,
      "grad_norm": 6.07615327835083,
      "learning_rate": 3.749034749034749e-05,
      "loss": 0.623,
      "step": 975
    },
    {
      "epoch": 1.9305019305019306,
      "grad_norm": 5.640119552612305,
      "learning_rate": 3.8455598455598456e-05,
      "loss": 0.6042,
      "step": 1000
    },
    {
      "epoch": 1.9787644787644787,
      "grad_norm": 4.701162338256836,
      "learning_rate": 3.942084942084942e-05,
      "loss": 0.6241,
      "step": 1025
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.8063241106719368,
      "eval_f1_weighted": 0.7973080602802811,
      "eval_loss": 0.512923538684845,
      "eval_runtime": 36.4194,
      "eval_samples_per_second": 90.309,
      "eval_steps_per_second": 0.961,
      "step": 1036
    },
    {
      "epoch": 2.027027027027027,
      "grad_norm": 4.633699417114258,
      "learning_rate": 4.038610038610039e-05,
      "loss": 0.5871,
      "step": 1050
    },
    {
      "epoch": 2.0752895752895753,
      "grad_norm": 6.390873432159424,
      "learning_rate": 4.135135135135135e-05,
      "loss": 0.5684,
      "step": 1075
    },
    {
      "epoch": 2.1235521235521237,
      "grad_norm": 6.305542469024658,
      "learning_rate": 4.2316602316602314e-05,
      "loss": 0.5302,
      "step": 1100
    },
    {
      "epoch": 2.171814671814672,
      "grad_norm": 5.2645583152771,
      "learning_rate": 4.3281853281853284e-05,
      "loss": 0.6071,
      "step": 1125
    },
    {
      "epoch": 2.2200772200772203,
      "grad_norm": 7.025975227355957,
      "learning_rate": 4.424710424710425e-05,
      "loss": 0.5658,
      "step": 1150
    },
    {
      "epoch": 2.2683397683397684,
      "grad_norm": 4.293776988983154,
      "learning_rate": 4.521235521235521e-05,
      "loss": 0.5419,
      "step": 1175
    },
    {
      "epoch": 2.3166023166023164,
      "grad_norm": 5.518339157104492,
      "learning_rate": 4.617760617760618e-05,
      "loss": 0.5818,
      "step": 1200
    },
    {
      "epoch": 2.364864864864865,
      "grad_norm": 5.215935707092285,
      "learning_rate": 4.714285714285714e-05,
      "loss": 0.6324,
      "step": 1225
    },
    {
      "epoch": 2.413127413127413,
      "grad_norm": 4.2902936935424805,
      "learning_rate": 4.810810810810811e-05,
      "loss": 0.5803,
      "step": 1250
    },
    {
      "epoch": 2.4613899613899615,
      "grad_norm": 5.62481689453125,
      "learning_rate": 4.9073359073359075e-05,
      "loss": 0.6019,
      "step": 1275
    },
    {
      "epoch": 2.5096525096525095,
      "grad_norm": 8.87509536743164,
      "learning_rate": 4.999570999571e-05,
      "loss": 0.5941,
      "step": 1300
    },
    {
      "epoch": 2.557915057915058,
      "grad_norm": 6.890492916107178,
      "learning_rate": 4.9888459888459895e-05,
      "loss": 0.5299,
      "step": 1325
    },
    {
      "epoch": 2.606177606177606,
      "grad_norm": 4.008423805236816,
      "learning_rate": 4.978120978120978e-05,
      "loss": 0.5605,
      "step": 1350
    },
    {
      "epoch": 2.6544401544401546,
      "grad_norm": 4.182531356811523,
      "learning_rate": 4.9673959673959674e-05,
      "loss": 0.6116,
      "step": 1375
    },
    {
      "epoch": 2.7027027027027026,
      "grad_norm": 4.860417366027832,
      "learning_rate": 4.956670956670957e-05,
      "loss": 0.595,
      "step": 1400
    },
    {
      "epoch": 2.750965250965251,
      "grad_norm": 5.1701812744140625,
      "learning_rate": 4.945945945945946e-05,
      "loss": 0.4975,
      "step": 1425
    },
    {
      "epoch": 2.799227799227799,
      "grad_norm": 4.421745300292969,
      "learning_rate": 4.9352209352209353e-05,
      "loss": 0.5827,
      "step": 1450
    },
    {
      "epoch": 2.8474903474903472,
      "grad_norm": 5.523265361785889,
      "learning_rate": 4.9244959244959246e-05,
      "loss": 0.585,
      "step": 1475
    },
    {
      "epoch": 2.8957528957528957,
      "grad_norm": 6.698300361633301,
      "learning_rate": 4.913770913770914e-05,
      "loss": 0.5643,
      "step": 1500
    },
    {
      "epoch": 2.9440154440154442,
      "grad_norm": 5.045505046844482,
      "learning_rate": 4.903045903045903e-05,
      "loss": 0.5724,
      "step": 1525
    },
    {
      "epoch": 2.9922779922779923,
      "grad_norm": 3.9405736923217773,
      "learning_rate": 4.8923208923208925e-05,
      "loss": 0.557,
      "step": 1550
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.8181818181818182,
      "eval_f1_weighted": 0.8120657771648743,
      "eval_loss": 0.48036712408065796,
      "eval_runtime": 36.2592,
      "eval_samples_per_second": 90.708,
      "eval_steps_per_second": 0.965,
      "step": 1554
    },
    {
      "epoch": 3.0405405405405403,
      "grad_norm": 4.885291576385498,
      "learning_rate": 4.881595881595882e-05,
      "loss": 0.5552,
      "step": 1575
    },
    {
      "epoch": 3.088803088803089,
      "grad_norm": 5.552316665649414,
      "learning_rate": 4.870870870870871e-05,
      "loss": 0.5238,
      "step": 1600
    },
    {
      "epoch": 3.137065637065637,
      "grad_norm": 3.8052306175231934,
      "learning_rate": 4.8601458601458604e-05,
      "loss": 0.5154,
      "step": 1625
    },
    {
      "epoch": 3.1853281853281854,
      "grad_norm": 6.777619361877441,
      "learning_rate": 4.84942084942085e-05,
      "loss": 0.5631,
      "step": 1650
    },
    {
      "epoch": 3.2335907335907335,
      "grad_norm": 4.203267574310303,
      "learning_rate": 4.838695838695839e-05,
      "loss": 0.5187,
      "step": 1675
    },
    {
      "epoch": 3.281853281853282,
      "grad_norm": 5.2231645584106445,
      "learning_rate": 4.827970827970828e-05,
      "loss": 0.5617,
      "step": 1700
    },
    {
      "epoch": 3.33011583011583,
      "grad_norm": 5.117671012878418,
      "learning_rate": 4.8172458172458177e-05,
      "loss": 0.4716,
      "step": 1725
    },
    {
      "epoch": 3.3783783783783785,
      "grad_norm": 5.566807746887207,
      "learning_rate": 4.806520806520807e-05,
      "loss": 0.5689,
      "step": 1750
    },
    {
      "epoch": 3.4266409266409266,
      "grad_norm": 4.726461410522461,
      "learning_rate": 4.795795795795796e-05,
      "loss": 0.5444,
      "step": 1775
    },
    {
      "epoch": 3.474903474903475,
      "grad_norm": 3.7846901416778564,
      "learning_rate": 4.7850707850707856e-05,
      "loss": 0.5452,
      "step": 1800
    },
    {
      "epoch": 3.523166023166023,
      "grad_norm": 10.869094848632812,
      "learning_rate": 4.774345774345775e-05,
      "loss": 0.4995,
      "step": 1825
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 4.366391181945801,
      "learning_rate": 4.7636207636207635e-05,
      "loss": 0.5714,
      "step": 1850
    },
    {
      "epoch": 3.6196911196911197,
      "grad_norm": 6.312164783477783,
      "learning_rate": 4.752895752895753e-05,
      "loss": 0.5421,
      "step": 1875
    },
    {
      "epoch": 3.667953667953668,
      "grad_norm": 5.794949054718018,
      "learning_rate": 4.742170742170742e-05,
      "loss": 0.5234,
      "step": 1900
    },
    {
      "epoch": 3.7162162162162162,
      "grad_norm": 6.849250793457031,
      "learning_rate": 4.7314457314457314e-05,
      "loss": 0.5538,
      "step": 1925
    },
    {
      "epoch": 3.7644787644787643,
      "grad_norm": 4.790961742401123,
      "learning_rate": 4.7207207207207214e-05,
      "loss": 0.5314,
      "step": 1950
    },
    {
      "epoch": 3.812741312741313,
      "grad_norm": 4.983593940734863,
      "learning_rate": 4.7099957099957107e-05,
      "loss": 0.5759,
      "step": 1975
    },
    {
      "epoch": 3.861003861003861,
      "grad_norm": 3.6010634899139404,
      "learning_rate": 4.6992706992707e-05,
      "loss": 0.5292,
      "step": 2000
    },
    {
      "epoch": 3.9092664092664093,
      "grad_norm": 4.362269401550293,
      "learning_rate": 4.6885456885456886e-05,
      "loss": 0.5652,
      "step": 2025
    },
    {
      "epoch": 3.9575289575289574,
      "grad_norm": 4.139570713043213,
      "learning_rate": 4.677820677820678e-05,
      "loss": 0.5698,
      "step": 2050
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.8291273943447857,
      "eval_f1_weighted": 0.8232080860647297,
      "eval_loss": 0.4689806401729584,
      "eval_runtime": 36.6694,
      "eval_samples_per_second": 89.693,
      "eval_steps_per_second": 0.954,
      "step": 2072
    },
    {
      "epoch": 4.005791505791506,
      "grad_norm": 3.949352741241455,
      "learning_rate": 4.667095667095667e-05,
      "loss": 0.4985,
      "step": 2075
    },
    {
      "epoch": 4.054054054054054,
      "grad_norm": 4.439121723175049,
      "learning_rate": 4.6563706563706565e-05,
      "loss": 0.5042,
      "step": 2100
    },
    {
      "epoch": 4.102316602316602,
      "grad_norm": 5.710211277008057,
      "learning_rate": 4.645645645645646e-05,
      "loss": 0.5376,
      "step": 2125
    },
    {
      "epoch": 4.1505791505791505,
      "grad_norm": 3.005373001098633,
      "learning_rate": 4.634920634920635e-05,
      "loss": 0.5027,
      "step": 2150
    },
    {
      "epoch": 4.198841698841699,
      "grad_norm": 5.926522731781006,
      "learning_rate": 4.6241956241956244e-05,
      "loss": 0.4669,
      "step": 2175
    },
    {
      "epoch": 4.2471042471042475,
      "grad_norm": 5.7040114402771,
      "learning_rate": 4.613470613470614e-05,
      "loss": 0.4578,
      "step": 2200
    },
    {
      "epoch": 4.295366795366795,
      "grad_norm": 4.997922897338867,
      "learning_rate": 4.602745602745603e-05,
      "loss": 0.5147,
      "step": 2225
    },
    {
      "epoch": 4.343629343629344,
      "grad_norm": 4.988228797912598,
      "learning_rate": 4.592020592020592e-05,
      "loss": 0.5214,
      "step": 2250
    },
    {
      "epoch": 4.391891891891892,
      "grad_norm": 4.0854973793029785,
      "learning_rate": 4.5812955812955816e-05,
      "loss": 0.5337,
      "step": 2275
    },
    {
      "epoch": 4.440154440154441,
      "grad_norm": 5.3923258781433105,
      "learning_rate": 4.570570570570571e-05,
      "loss": 0.5376,
      "step": 2300
    },
    {
      "epoch": 4.488416988416988,
      "grad_norm": 4.388210296630859,
      "learning_rate": 4.55984555984556e-05,
      "loss": 0.4887,
      "step": 2325
    },
    {
      "epoch": 4.536679536679537,
      "grad_norm": 4.107499599456787,
      "learning_rate": 4.549120549120549e-05,
      "loss": 0.586,
      "step": 2350
    },
    {
      "epoch": 4.584942084942085,
      "grad_norm": 2.5173683166503906,
      "learning_rate": 4.538395538395538e-05,
      "loss": 0.4585,
      "step": 2375
    },
    {
      "epoch": 4.633204633204633,
      "grad_norm": 3.477450370788574,
      "learning_rate": 4.527670527670528e-05,
      "loss": 0.4841,
      "step": 2400
    },
    {
      "epoch": 4.681467181467181,
      "grad_norm": 5.77752161026001,
      "learning_rate": 4.5169455169455174e-05,
      "loss": 0.4999,
      "step": 2425
    },
    {
      "epoch": 4.72972972972973,
      "grad_norm": 3.749601125717163,
      "learning_rate": 4.506220506220507e-05,
      "loss": 0.5247,
      "step": 2450
    },
    {
      "epoch": 4.777992277992278,
      "grad_norm": 5.589895725250244,
      "learning_rate": 4.495495495495496e-05,
      "loss": 0.5334,
      "step": 2475
    },
    {
      "epoch": 4.826254826254826,
      "grad_norm": 3.2330257892608643,
      "learning_rate": 4.484770484770485e-05,
      "loss": 0.4655,
      "step": 2500
    },
    {
      "epoch": 4.874517374517374,
      "grad_norm": 3.757375717163086,
      "learning_rate": 4.474045474045474e-05,
      "loss": 0.5307,
      "step": 2525
    },
    {
      "epoch": 4.922779922779923,
      "grad_norm": 3.5954461097717285,
      "learning_rate": 4.463320463320463e-05,
      "loss": 0.4932,
      "step": 2550
    },
    {
      "epoch": 4.971042471042471,
      "grad_norm": 4.616725921630859,
      "learning_rate": 4.4525954525954525e-05,
      "loss": 0.4751,
      "step": 2575
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.8391608391608392,
      "eval_f1_weighted": 0.8382315375284795,
      "eval_loss": 0.43535974621772766,
      "eval_runtime": 36.5918,
      "eval_samples_per_second": 89.884,
      "eval_steps_per_second": 0.956,
      "step": 2590
    },
    {
      "epoch": 5.019305019305019,
      "grad_norm": 5.341448783874512,
      "learning_rate": 4.441870441870442e-05,
      "loss": 0.4651,
      "step": 2600
    },
    {
      "epoch": 5.0675675675675675,
      "grad_norm": 3.063326597213745,
      "learning_rate": 4.431145431145432e-05,
      "loss": 0.4875,
      "step": 2625
    },
    {
      "epoch": 5.115830115830116,
      "grad_norm": 5.316182613372803,
      "learning_rate": 4.420420420420421e-05,
      "loss": 0.4861,
      "step": 2650
    },
    {
      "epoch": 5.164092664092664,
      "grad_norm": 4.188124656677246,
      "learning_rate": 4.4096954096954104e-05,
      "loss": 0.4882,
      "step": 2675
    },
    {
      "epoch": 5.212355212355212,
      "grad_norm": 4.736623764038086,
      "learning_rate": 4.398970398970399e-05,
      "loss": 0.4675,
      "step": 2700
    },
    {
      "epoch": 5.260617760617761,
      "grad_norm": 3.452396869659424,
      "learning_rate": 4.388245388245388e-05,
      "loss": 0.4169,
      "step": 2725
    },
    {
      "epoch": 5.308880308880309,
      "grad_norm": 6.058547496795654,
      "learning_rate": 4.3775203775203776e-05,
      "loss": 0.4753,
      "step": 2750
    },
    {
      "epoch": 5.357142857142857,
      "grad_norm": 3.4183335304260254,
      "learning_rate": 4.366795366795367e-05,
      "loss": 0.4607,
      "step": 2775
    },
    {
      "epoch": 5.405405405405405,
      "grad_norm": 3.407829761505127,
      "learning_rate": 4.356070356070356e-05,
      "loss": 0.4932,
      "step": 2800
    },
    {
      "epoch": 5.453667953667954,
      "grad_norm": 4.577021598815918,
      "learning_rate": 4.3453453453453455e-05,
      "loss": 0.4418,
      "step": 2825
    },
    {
      "epoch": 5.501930501930502,
      "grad_norm": 4.055690765380859,
      "learning_rate": 4.334620334620335e-05,
      "loss": 0.4979,
      "step": 2850
    },
    {
      "epoch": 5.55019305019305,
      "grad_norm": 5.494040012359619,
      "learning_rate": 4.323895323895324e-05,
      "loss": 0.4862,
      "step": 2875
    },
    {
      "epoch": 5.598455598455598,
      "grad_norm": 7.553224086761475,
      "learning_rate": 4.3131703131703134e-05,
      "loss": 0.4894,
      "step": 2900
    },
    {
      "epoch": 5.646718146718147,
      "grad_norm": 5.205236911773682,
      "learning_rate": 4.302445302445303e-05,
      "loss": 0.4921,
      "step": 2925
    },
    {
      "epoch": 5.694980694980695,
      "grad_norm": 6.054269790649414,
      "learning_rate": 4.291720291720292e-05,
      "loss": 0.5242,
      "step": 2950
    },
    {
      "epoch": 5.743243243243243,
      "grad_norm": 6.362863540649414,
      "learning_rate": 4.280995280995281e-05,
      "loss": 0.4016,
      "step": 2975
    },
    {
      "epoch": 5.7915057915057915,
      "grad_norm": 5.2200236320495605,
      "learning_rate": 4.2702702702702706e-05,
      "loss": 0.5207,
      "step": 3000
    },
    {
      "epoch": 5.83976833976834,
      "grad_norm": 5.377509117126465,
      "learning_rate": 4.259545259545259e-05,
      "loss": 0.458,
      "step": 3025
    },
    {
      "epoch": 5.8880308880308885,
      "grad_norm": 4.403051376342773,
      "learning_rate": 4.2488202488202486e-05,
      "loss": 0.4446,
      "step": 3050
    },
    {
      "epoch": 5.936293436293436,
      "grad_norm": 4.245752334594727,
      "learning_rate": 4.2380952380952385e-05,
      "loss": 0.4966,
      "step": 3075
    },
    {
      "epoch": 5.984555984555985,
      "grad_norm": 5.2902021408081055,
      "learning_rate": 4.227370227370228e-05,
      "loss": 0.5129,
      "step": 3100
    },
    {
      "epoch": 6.0,
      "eval_accuracy": 0.8431134083307996,
      "eval_f1_weighted": 0.8377558963413199,
      "eval_loss": 0.43674033880233765,
      "eval_runtime": 36.7069,
      "eval_samples_per_second": 89.602,
      "eval_steps_per_second": 0.953,
      "step": 3108
    },
    {
      "epoch": 6.032818532818533,
      "grad_norm": 4.29031229019165,
      "learning_rate": 4.216645216645217e-05,
      "loss": 0.4287,
      "step": 3125
    },
    {
      "epoch": 6.081081081081081,
      "grad_norm": 5.240657806396484,
      "learning_rate": 4.2059202059202064e-05,
      "loss": 0.4866,
      "step": 3150
    },
    {
      "epoch": 6.129343629343629,
      "grad_norm": 5.511458396911621,
      "learning_rate": 4.195195195195196e-05,
      "loss": 0.4083,
      "step": 3175
    },
    {
      "epoch": 6.177606177606178,
      "grad_norm": 4.808923244476318,
      "learning_rate": 4.1844701844701844e-05,
      "loss": 0.4562,
      "step": 3200
    },
    {
      "epoch": 6.225868725868726,
      "grad_norm": 5.494490623474121,
      "learning_rate": 4.1737451737451737e-05,
      "loss": 0.4286,
      "step": 3225
    },
    {
      "epoch": 6.274131274131274,
      "grad_norm": 6.531881809234619,
      "learning_rate": 4.163020163020163e-05,
      "loss": 0.4643,
      "step": 3250
    },
    {
      "epoch": 6.322393822393822,
      "grad_norm": 7.233857154846191,
      "learning_rate": 4.152295152295152e-05,
      "loss": 0.4237,
      "step": 3275
    },
    {
      "epoch": 6.370656370656371,
      "grad_norm": 4.43874979019165,
      "learning_rate": 4.1415701415701416e-05,
      "loss": 0.3937,
      "step": 3300
    },
    {
      "epoch": 6.418918918918919,
      "grad_norm": 6.7851176261901855,
      "learning_rate": 4.1308451308451315e-05,
      "loss": 0.4307,
      "step": 3325
    },
    {
      "epoch": 6.467181467181467,
      "grad_norm": 6.45835018157959,
      "learning_rate": 4.12012012012012e-05,
      "loss": 0.4683,
      "step": 3350
    },
    {
      "epoch": 6.515444015444015,
      "grad_norm": 3.3506104946136475,
      "learning_rate": 4.1093951093951095e-05,
      "loss": 0.4388,
      "step": 3375
    },
    {
      "epoch": 6.563706563706564,
      "grad_norm": 4.404556751251221,
      "learning_rate": 4.098670098670099e-05,
      "loss": 0.5215,
      "step": 3400
    },
    {
      "epoch": 6.6119691119691115,
      "grad_norm": 2.929647445678711,
      "learning_rate": 4.087945087945088e-05,
      "loss": 0.4033,
      "step": 3425
    },
    {
      "epoch": 6.66023166023166,
      "grad_norm": 5.641740798950195,
      "learning_rate": 4.0772200772200774e-05,
      "loss": 0.449,
      "step": 3450
    },
    {
      "epoch": 6.7084942084942085,
      "grad_norm": 3.9634475708007812,
      "learning_rate": 4.066495066495067e-05,
      "loss": 0.4298,
      "step": 3475
    },
    {
      "epoch": 6.756756756756757,
      "grad_norm": 4.851713180541992,
      "learning_rate": 4.055770055770056e-05,
      "loss": 0.4634,
      "step": 3500
    },
    {
      "epoch": 6.805019305019305,
      "grad_norm": 4.505391597747803,
      "learning_rate": 4.045045045045045e-05,
      "loss": 0.4595,
      "step": 3525
    },
    {
      "epoch": 6.853281853281853,
      "grad_norm": 4.7980570793151855,
      "learning_rate": 4.0343200343200346e-05,
      "loss": 0.4917,
      "step": 3550
    },
    {
      "epoch": 6.901544401544402,
      "grad_norm": 5.571117401123047,
      "learning_rate": 4.023595023595024e-05,
      "loss": 0.4458,
      "step": 3575
    },
    {
      "epoch": 6.94980694980695,
      "grad_norm": 4.713160037994385,
      "learning_rate": 4.012870012870013e-05,
      "loss": 0.4518,
      "step": 3600
    },
    {
      "epoch": 6.998069498069498,
      "grad_norm": 8.552634239196777,
      "learning_rate": 4.0021450021450025e-05,
      "loss": 0.4408,
      "step": 3625
    },
    {
      "epoch": 7.0,
      "eval_accuracy": 0.8346001824262694,
      "eval_f1_weighted": 0.8294770624364581,
      "eval_loss": 0.4726608395576477,
      "eval_runtime": 36.9093,
      "eval_samples_per_second": 89.11,
      "eval_steps_per_second": 0.948,
      "step": 3626
    },
    {
      "epoch": 7.046332046332046,
      "grad_norm": 4.657142162322998,
      "learning_rate": 3.991419991419992e-05,
      "loss": 0.4254,
      "step": 3650
    },
    {
      "epoch": 7.094594594594595,
      "grad_norm": 4.429532527923584,
      "learning_rate": 3.980694980694981e-05,
      "loss": 0.3961,
      "step": 3675
    },
    {
      "epoch": 7.142857142857143,
      "grad_norm": 5.085684776306152,
      "learning_rate": 3.96996996996997e-05,
      "loss": 0.399,
      "step": 3700
    },
    {
      "epoch": 7.191119691119691,
      "grad_norm": 7.890208721160889,
      "learning_rate": 3.959244959244959e-05,
      "loss": 0.3607,
      "step": 3725
    },
    {
      "epoch": 7.239382239382239,
      "grad_norm": 5.766313552856445,
      "learning_rate": 3.948519948519949e-05,
      "loss": 0.4194,
      "step": 3750
    },
    {
      "epoch": 7.287644787644788,
      "grad_norm": 8.525172233581543,
      "learning_rate": 3.937794937794938e-05,
      "loss": 0.4414,
      "step": 3775
    },
    {
      "epoch": 7.335907335907336,
      "grad_norm": 4.422700881958008,
      "learning_rate": 3.9270699270699276e-05,
      "loss": 0.4041,
      "step": 3800
    },
    {
      "epoch": 7.384169884169884,
      "grad_norm": 4.219031810760498,
      "learning_rate": 3.916344916344917e-05,
      "loss": 0.4612,
      "step": 3825
    },
    {
      "epoch": 7.4324324324324325,
      "grad_norm": 5.069619655609131,
      "learning_rate": 3.9056199056199055e-05,
      "loss": 0.4541,
      "step": 3850
    },
    {
      "epoch": 7.480694980694981,
      "grad_norm": 6.168966770172119,
      "learning_rate": 3.894894894894895e-05,
      "loss": 0.4121,
      "step": 3875
    },
    {
      "epoch": 7.528957528957529,
      "grad_norm": 3.5482888221740723,
      "learning_rate": 3.884169884169884e-05,
      "loss": 0.4401,
      "step": 3900
    },
    {
      "epoch": 7.577220077220077,
      "grad_norm": 4.025832176208496,
      "learning_rate": 3.8734448734448734e-05,
      "loss": 0.3541,
      "step": 3925
    },
    {
      "epoch": 7.625482625482626,
      "grad_norm": 4.136091709136963,
      "learning_rate": 3.862719862719863e-05,
      "loss": 0.3893,
      "step": 3950
    },
    {
      "epoch": 7.673745173745174,
      "grad_norm": 4.7550554275512695,
      "learning_rate": 3.851994851994852e-05,
      "loss": 0.4306,
      "step": 3975
    },
    {
      "epoch": 7.722007722007722,
      "grad_norm": 4.974767684936523,
      "learning_rate": 3.841269841269842e-05,
      "loss": 0.4568,
      "step": 4000
    },
    {
      "epoch": 7.77027027027027,
      "grad_norm": 5.650887966156006,
      "learning_rate": 3.8305448305448306e-05,
      "loss": 0.4355,
      "step": 4025
    },
    {
      "epoch": 7.818532818532819,
      "grad_norm": 6.981386661529541,
      "learning_rate": 3.81981981981982e-05,
      "loss": 0.4239,
      "step": 4050
    },
    {
      "epoch": 7.866795366795367,
      "grad_norm": 3.316885232925415,
      "learning_rate": 3.809094809094809e-05,
      "loss": 0.4206,
      "step": 4075
    },
    {
      "epoch": 7.915057915057915,
      "grad_norm": 6.536417484283447,
      "learning_rate": 3.7983697983697985e-05,
      "loss": 0.4688,
      "step": 4100
    },
    {
      "epoch": 7.963320463320463,
      "grad_norm": 6.386332988739014,
      "learning_rate": 3.787644787644788e-05,
      "loss": 0.4063,
      "step": 4125
    },
    {
      "epoch": 8.0,
      "eval_accuracy": 0.8437214958954089,
      "eval_f1_weighted": 0.8432786025587645,
      "eval_loss": 0.44400379061698914,
      "eval_runtime": 36.5036,
      "eval_samples_per_second": 90.101,
      "eval_steps_per_second": 0.959,
      "step": 4144
    },
    {
      "epoch": 8.011583011583012,
      "grad_norm": 7.139015197753906,
      "learning_rate": 3.776919776919777e-05,
      "loss": 0.3802,
      "step": 4150
    },
    {
      "epoch": 8.05984555984556,
      "grad_norm": 6.378098964691162,
      "learning_rate": 3.7661947661947664e-05,
      "loss": 0.3917,
      "step": 4175
    },
    {
      "epoch": 8.108108108108109,
      "grad_norm": 6.089890003204346,
      "learning_rate": 3.755469755469756e-05,
      "loss": 0.3785,
      "step": 4200
    },
    {
      "epoch": 8.156370656370656,
      "grad_norm": 6.29162073135376,
      "learning_rate": 3.744744744744745e-05,
      "loss": 0.3747,
      "step": 4225
    },
    {
      "epoch": 8.204633204633204,
      "grad_norm": 4.408153057098389,
      "learning_rate": 3.734019734019734e-05,
      "loss": 0.4075,
      "step": 4250
    },
    {
      "epoch": 8.252895752895753,
      "grad_norm": 5.662875175476074,
      "learning_rate": 3.7232947232947236e-05,
      "loss": 0.4171,
      "step": 4275
    },
    {
      "epoch": 8.301158301158301,
      "grad_norm": 3.0756664276123047,
      "learning_rate": 3.712569712569713e-05,
      "loss": 0.3763,
      "step": 4300
    },
    {
      "epoch": 8.349420849420849,
      "grad_norm": 4.996885776519775,
      "learning_rate": 3.701844701844702e-05,
      "loss": 0.4038,
      "step": 4325
    },
    {
      "epoch": 8.397683397683398,
      "grad_norm": 4.187341690063477,
      "learning_rate": 3.691119691119691e-05,
      "loss": 0.3654,
      "step": 4350
    },
    {
      "epoch": 8.445945945945946,
      "grad_norm": 5.550887107849121,
      "learning_rate": 3.68039468039468e-05,
      "loss": 0.4026,
      "step": 4375
    },
    {
      "epoch": 8.494208494208495,
      "grad_norm": 4.703464031219482,
      "learning_rate": 3.6696696696696694e-05,
      "loss": 0.3932,
      "step": 4400
    },
    {
      "epoch": 8.542471042471043,
      "grad_norm": 6.247624397277832,
      "learning_rate": 3.6589446589446594e-05,
      "loss": 0.4069,
      "step": 4425
    },
    {
      "epoch": 8.59073359073359,
      "grad_norm": 5.172643661499023,
      "learning_rate": 3.648219648219649e-05,
      "loss": 0.4386,
      "step": 4450
    },
    {
      "epoch": 8.63899613899614,
      "grad_norm": 7.371511936187744,
      "learning_rate": 3.637494637494638e-05,
      "loss": 0.4123,
      "step": 4475
    },
    {
      "epoch": 8.687258687258687,
      "grad_norm": 7.798197269439697,
      "learning_rate": 3.626769626769627e-05,
      "loss": 0.3853,
      "step": 4500
    },
    {
      "epoch": 8.735521235521235,
      "grad_norm": 5.20034646987915,
      "learning_rate": 3.616044616044616e-05,
      "loss": 0.383,
      "step": 4525
    },
    {
      "epoch": 8.783783783783784,
      "grad_norm": 6.852813243865967,
      "learning_rate": 3.605319605319605e-05,
      "loss": 0.3943,
      "step": 4550
    },
    {
      "epoch": 8.832046332046332,
      "grad_norm": 4.507845878601074,
      "learning_rate": 3.5945945945945945e-05,
      "loss": 0.436,
      "step": 4575
    },
    {
      "epoch": 8.880308880308881,
      "grad_norm": 6.677669048309326,
      "learning_rate": 3.583869583869584e-05,
      "loss": 0.4273,
      "step": 4600
    },
    {
      "epoch": 8.928571428571429,
      "grad_norm": 5.06051778793335,
      "learning_rate": 3.573144573144573e-05,
      "loss": 0.4322,
      "step": 4625
    },
    {
      "epoch": 8.976833976833976,
      "grad_norm": 5.450156211853027,
      "learning_rate": 3.5624195624195624e-05,
      "loss": 0.3848,
      "step": 4650
    },
    {
      "epoch": 9.0,
      "eval_accuracy": 0.8473700212830647,
      "eval_f1_weighted": 0.8434450079922878,
      "eval_loss": 0.4413984417915344,
      "eval_runtime": 36.337,
      "eval_samples_per_second": 90.514,
      "eval_steps_per_second": 0.963,
      "step": 4662
    },
    {
      "epoch": 9.025096525096526,
      "grad_norm": 4.64163064956665,
      "learning_rate": 3.5516945516945524e-05,
      "loss": 0.3868,
      "step": 4675
    },
    {
      "epoch": 9.073359073359073,
      "grad_norm": 5.991057872772217,
      "learning_rate": 3.541398541398542e-05,
      "loss": 0.4129,
      "step": 4700
    },
    {
      "epoch": 9.121621621621621,
      "grad_norm": 6.283178329467773,
      "learning_rate": 3.5306735306735304e-05,
      "loss": 0.3918,
      "step": 4725
    },
    {
      "epoch": 9.16988416988417,
      "grad_norm": 2.4273765087127686,
      "learning_rate": 3.51994851994852e-05,
      "loss": 0.3927,
      "step": 4750
    },
    {
      "epoch": 9.218146718146718,
      "grad_norm": 4.683492660522461,
      "learning_rate": 3.509223509223509e-05,
      "loss": 0.3656,
      "step": 4775
    },
    {
      "epoch": 9.266409266409266,
      "grad_norm": 4.21309232711792,
      "learning_rate": 3.498498498498499e-05,
      "loss": 0.3515,
      "step": 4800
    },
    {
      "epoch": 9.314671814671815,
      "grad_norm": 9.528399467468262,
      "learning_rate": 3.487773487773488e-05,
      "loss": 0.3543,
      "step": 4825
    },
    {
      "epoch": 9.362934362934363,
      "grad_norm": 6.0111565589904785,
      "learning_rate": 3.4770484770484775e-05,
      "loss": 0.3659,
      "step": 4850
    },
    {
      "epoch": 9.411196911196912,
      "grad_norm": 8.026300430297852,
      "learning_rate": 3.466323466323467e-05,
      "loss": 0.3852,
      "step": 4875
    },
    {
      "epoch": 9.45945945945946,
      "grad_norm": 5.012167930603027,
      "learning_rate": 3.4555984555984555e-05,
      "loss": 0.362,
      "step": 4900
    },
    {
      "epoch": 9.507722007722007,
      "grad_norm": 4.225951194763184,
      "learning_rate": 3.444873444873445e-05,
      "loss": 0.3704,
      "step": 4925
    },
    {
      "epoch": 9.555984555984557,
      "grad_norm": 9.8408203125,
      "learning_rate": 3.434148434148434e-05,
      "loss": 0.3712,
      "step": 4950
    },
    {
      "epoch": 9.604247104247104,
      "grad_norm": 4.2620158195495605,
      "learning_rate": 3.4234234234234234e-05,
      "loss": 0.3465,
      "step": 4975
    },
    {
      "epoch": 9.652509652509652,
      "grad_norm": 11.132458686828613,
      "learning_rate": 3.412698412698413e-05,
      "loss": 0.4383,
      "step": 5000
    },
    {
      "epoch": 9.700772200772201,
      "grad_norm": 3.4516959190368652,
      "learning_rate": 3.401973401973402e-05,
      "loss": 0.3876,
      "step": 5025
    },
    {
      "epoch": 9.749034749034749,
      "grad_norm": 4.526008605957031,
      "learning_rate": 3.391248391248392e-05,
      "loss": 0.3528,
      "step": 5050
    },
    {
      "epoch": 9.797297297297296,
      "grad_norm": 3.546135187149048,
      "learning_rate": 3.3805233805233806e-05,
      "loss": 0.3825,
      "step": 5075
    },
    {
      "epoch": 9.845559845559846,
      "grad_norm": 7.6580119132995605,
      "learning_rate": 3.36979836979837e-05,
      "loss": 0.3765,
      "step": 5100
    },
    {
      "epoch": 9.893822393822393,
      "grad_norm": 8.345542907714844,
      "learning_rate": 3.359073359073359e-05,
      "loss": 0.3751,
      "step": 5125
    },
    {
      "epoch": 9.942084942084943,
      "grad_norm": 5.52201509475708,
      "learning_rate": 3.3483483483483485e-05,
      "loss": 0.4129,
      "step": 5150
    },
    {
      "epoch": 9.99034749034749,
      "grad_norm": 5.86322546005249,
      "learning_rate": 3.337623337623338e-05,
      "loss": 0.3814,
      "step": 5175
    },
    {
      "epoch": 10.0,
      "eval_accuracy": 0.8443295834600183,
      "eval_f1_weighted": 0.8405193654998939,
      "eval_loss": 0.4749637544155121,
      "eval_runtime": 36.6608,
      "eval_samples_per_second": 89.714,
      "eval_steps_per_second": 0.955,
      "step": 5180
    },
    {
      "epoch": 10.038610038610038,
      "grad_norm": 8.84369945526123,
      "learning_rate": 3.326898326898327e-05,
      "loss": 0.3211,
      "step": 5200
    },
    {
      "epoch": 10.086872586872587,
      "grad_norm": 3.895230531692505,
      "learning_rate": 3.3161733161733164e-05,
      "loss": 0.3797,
      "step": 5225
    },
    {
      "epoch": 10.135135135135135,
      "grad_norm": 6.098042011260986,
      "learning_rate": 3.305448305448306e-05,
      "loss": 0.3144,
      "step": 5250
    },
    {
      "epoch": 10.183397683397683,
      "grad_norm": 9.003103256225586,
      "learning_rate": 3.294723294723295e-05,
      "loss": 0.3499,
      "step": 5275
    },
    {
      "epoch": 10.231660231660232,
      "grad_norm": 6.155412197113037,
      "learning_rate": 3.283998283998284e-05,
      "loss": 0.3847,
      "step": 5300
    },
    {
      "epoch": 10.27992277992278,
      "grad_norm": 6.327291488647461,
      "learning_rate": 3.2732732732732736e-05,
      "loss": 0.372,
      "step": 5325
    },
    {
      "epoch": 10.328185328185327,
      "grad_norm": 4.544366836547852,
      "learning_rate": 3.262548262548263e-05,
      "loss": 0.3579,
      "step": 5350
    },
    {
      "epoch": 10.376447876447877,
      "grad_norm": 9.671759605407715,
      "learning_rate": 3.251823251823252e-05,
      "loss": 0.3433,
      "step": 5375
    },
    {
      "epoch": 10.424710424710424,
      "grad_norm": 4.578813552856445,
      "learning_rate": 3.241098241098241e-05,
      "loss": 0.3154,
      "step": 5400
    },
    {
      "epoch": 10.472972972972974,
      "grad_norm": 10.54283618927002,
      "learning_rate": 3.23037323037323e-05,
      "loss": 0.3334,
      "step": 5425
    },
    {
      "epoch": 10.521235521235521,
      "grad_norm": 4.193259239196777,
      "learning_rate": 3.2196482196482194e-05,
      "loss": 0.3155,
      "step": 5450
    },
    {
      "epoch": 10.569498069498069,
      "grad_norm": 5.532510757446289,
      "learning_rate": 3.2089232089232094e-05,
      "loss": 0.3176,
      "step": 5475
    },
    {
      "epoch": 10.617760617760618,
      "grad_norm": 7.637578964233398,
      "learning_rate": 3.198198198198199e-05,
      "loss": 0.3417,
      "step": 5500
    },
    {
      "epoch": 10.666023166023166,
      "grad_norm": 7.156907558441162,
      "learning_rate": 3.187473187473188e-05,
      "loss": 0.3626,
      "step": 5525
    },
    {
      "epoch": 10.714285714285714,
      "grad_norm": 6.405214786529541,
      "learning_rate": 3.176748176748177e-05,
      "loss": 0.3738,
      "step": 5550
    },
    {
      "epoch": 10.762548262548263,
      "grad_norm": 5.571609973907471,
      "learning_rate": 3.166023166023166e-05,
      "loss": 0.3247,
      "step": 5575
    },
    {
      "epoch": 10.81081081081081,
      "grad_norm": 7.358552932739258,
      "learning_rate": 3.155298155298155e-05,
      "loss": 0.3687,
      "step": 5600
    },
    {
      "epoch": 10.85907335907336,
      "grad_norm": 3.9103002548217773,
      "learning_rate": 3.1445731445731445e-05,
      "loss": 0.3033,
      "step": 5625
    },
    {
      "epoch": 10.907335907335908,
      "grad_norm": 3.5194947719573975,
      "learning_rate": 3.133848133848134e-05,
      "loss": 0.3263,
      "step": 5650
    },
    {
      "epoch": 10.955598455598455,
      "grad_norm": 6.857151985168457,
      "learning_rate": 3.123123123123123e-05,
      "loss": 0.3784,
      "step": 5675
    },
    {
      "epoch": 11.0,
      "eval_accuracy": 0.8376406202493158,
      "eval_f1_weighted": 0.8338071531332925,
      "eval_loss": 0.5332000255584717,
      "eval_runtime": 36.598,
      "eval_samples_per_second": 89.868,
      "eval_steps_per_second": 0.956,
      "step": 5698
    },
    {
      "epoch": 11.003861003861005,
      "grad_norm": 8.601093292236328,
      "learning_rate": 3.1123981123981124e-05,
      "loss": 0.3303,
      "step": 5700
    },
    {
      "epoch": 11.052123552123552,
      "grad_norm": 8.010319709777832,
      "learning_rate": 3.101673101673102e-05,
      "loss": 0.3563,
      "step": 5725
    },
    {
      "epoch": 11.1003861003861,
      "grad_norm": 8.253751754760742,
      "learning_rate": 3.090948090948091e-05,
      "loss": 0.3767,
      "step": 5750
    },
    {
      "epoch": 11.14864864864865,
      "grad_norm": 3.4533796310424805,
      "learning_rate": 3.08022308022308e-05,
      "loss": 0.356,
      "step": 5775
    },
    {
      "epoch": 11.196911196911197,
      "grad_norm": 10.222110748291016,
      "learning_rate": 3.0694980694980696e-05,
      "loss": 0.298,
      "step": 5800
    },
    {
      "epoch": 11.245173745173744,
      "grad_norm": 3.4810903072357178,
      "learning_rate": 3.058773058773059e-05,
      "loss": 0.3372,
      "step": 5825
    },
    {
      "epoch": 11.293436293436294,
      "grad_norm": 8.984262466430664,
      "learning_rate": 3.0480480480480482e-05,
      "loss": 0.3397,
      "step": 5850
    },
    {
      "epoch": 11.341698841698841,
      "grad_norm": 6.592984676361084,
      "learning_rate": 3.037323037323038e-05,
      "loss": 0.2957,
      "step": 5875
    },
    {
      "epoch": 11.38996138996139,
      "grad_norm": 3.9026784896850586,
      "learning_rate": 3.0265980265980265e-05,
      "loss": 0.3318,
      "step": 5900
    },
    {
      "epoch": 11.438223938223938,
      "grad_norm": 7.066210746765137,
      "learning_rate": 3.0158730158730158e-05,
      "loss": 0.3115,
      "step": 5925
    },
    {
      "epoch": 11.486486486486486,
      "grad_norm": 5.57718563079834,
      "learning_rate": 3.005148005148005e-05,
      "loss": 0.3159,
      "step": 5950
    },
    {
      "epoch": 11.534749034749035,
      "grad_norm": 4.625773906707764,
      "learning_rate": 2.9944229944229947e-05,
      "loss": 0.3383,
      "step": 5975
    },
    {
      "epoch": 11.583011583011583,
      "grad_norm": 7.1419806480407715,
      "learning_rate": 2.983697983697984e-05,
      "loss": 0.3163,
      "step": 6000
    },
    {
      "epoch": 11.63127413127413,
      "grad_norm": 5.983444690704346,
      "learning_rate": 2.9729729729729733e-05,
      "loss": 0.2921,
      "step": 6025
    },
    {
      "epoch": 11.67953667953668,
      "grad_norm": 4.820825099945068,
      "learning_rate": 2.9622479622479626e-05,
      "loss": 0.3469,
      "step": 6050
    },
    {
      "epoch": 11.727799227799228,
      "grad_norm": 5.045192718505859,
      "learning_rate": 2.9515229515229516e-05,
      "loss": 0.3777,
      "step": 6075
    },
    {
      "epoch": 11.776061776061777,
      "grad_norm": 3.353477716445923,
      "learning_rate": 2.940797940797941e-05,
      "loss": 0.2949,
      "step": 6100
    },
    {
      "epoch": 11.824324324324325,
      "grad_norm": 7.750370502471924,
      "learning_rate": 2.9300729300729302e-05,
      "loss": 0.3741,
      "step": 6125
    },
    {
      "epoch": 11.872586872586872,
      "grad_norm": 5.470212459564209,
      "learning_rate": 2.9193479193479195e-05,
      "loss": 0.3475,
      "step": 6150
    },
    {
      "epoch": 11.920849420849422,
      "grad_norm": 5.6688432693481445,
      "learning_rate": 2.9086229086229088e-05,
      "loss": 0.3257,
      "step": 6175
    },
    {
      "epoch": 11.96911196911197,
      "grad_norm": 6.213352203369141,
      "learning_rate": 2.897897897897898e-05,
      "loss": 0.3008,
      "step": 6200
    },
    {
      "epoch": 12.0,
      "eval_accuracy": 0.8461538461538461,
      "eval_f1_weighted": 0.8448571153431302,
      "eval_loss": 0.5123542547225952,
      "eval_runtime": 36.8341,
      "eval_samples_per_second": 89.292,
      "eval_steps_per_second": 0.95,
      "step": 6216
    },
    {
      "epoch": 12.017374517374517,
      "grad_norm": 2.5598666667938232,
      "learning_rate": 2.8871728871728877e-05,
      "loss": 0.3216,
      "step": 6225
    },
    {
      "epoch": 12.065637065637066,
      "grad_norm": 8.623661994934082,
      "learning_rate": 2.8764478764478763e-05,
      "loss": 0.2851,
      "step": 6250
    },
    {
      "epoch": 12.113899613899614,
      "grad_norm": 6.38433837890625,
      "learning_rate": 2.8657228657228656e-05,
      "loss": 0.3011,
      "step": 6275
    },
    {
      "epoch": 12.162162162162161,
      "grad_norm": 10.229726791381836,
      "learning_rate": 2.854997854997855e-05,
      "loss": 0.2924,
      "step": 6300
    },
    {
      "epoch": 12.21042471042471,
      "grad_norm": 8.158116340637207,
      "learning_rate": 2.8442728442728446e-05,
      "loss": 0.3053,
      "step": 6325
    },
    {
      "epoch": 12.258687258687258,
      "grad_norm": 4.8153204917907715,
      "learning_rate": 2.833547833547834e-05,
      "loss": 0.3112,
      "step": 6350
    },
    {
      "epoch": 12.306949806949808,
      "grad_norm": 5.25247859954834,
      "learning_rate": 2.8228228228228232e-05,
      "loss": 0.3215,
      "step": 6375
    },
    {
      "epoch": 12.355212355212355,
      "grad_norm": 9.94218921661377,
      "learning_rate": 2.8120978120978118e-05,
      "loss": 0.2881,
      "step": 6400
    },
    {
      "epoch": 12.403474903474903,
      "grad_norm": 8.292778015136719,
      "learning_rate": 2.8013728013728014e-05,
      "loss": 0.3597,
      "step": 6425
    },
    {
      "epoch": 12.451737451737452,
      "grad_norm": 5.545376777648926,
      "learning_rate": 2.7906477906477908e-05,
      "loss": 0.3358,
      "step": 6450
    },
    {
      "epoch": 12.5,
      "grad_norm": 6.249202728271484,
      "learning_rate": 2.77992277992278e-05,
      "loss": 0.288,
      "step": 6475
    },
    {
      "epoch": 12.548262548262548,
      "grad_norm": 6.683791637420654,
      "learning_rate": 2.7691977691977694e-05,
      "loss": 0.2499,
      "step": 6500
    },
    {
      "epoch": 12.596525096525097,
      "grad_norm": 11.513642311096191,
      "learning_rate": 2.7584727584727587e-05,
      "loss": 0.3137,
      "step": 6525
    },
    {
      "epoch": 12.644787644787645,
      "grad_norm": 9.393437385559082,
      "learning_rate": 2.7477477477477483e-05,
      "loss": 0.2985,
      "step": 6550
    },
    {
      "epoch": 12.693050193050192,
      "grad_norm": 3.5434622764587402,
      "learning_rate": 2.737022737022737e-05,
      "loss": 0.2893,
      "step": 6575
    },
    {
      "epoch": 12.741312741312742,
      "grad_norm": 5.953404903411865,
      "learning_rate": 2.7262977262977262e-05,
      "loss": 0.3167,
      "step": 6600
    },
    {
      "epoch": 12.78957528957529,
      "grad_norm": 4.736663341522217,
      "learning_rate": 2.7155727155727155e-05,
      "loss": 0.3079,
      "step": 6625
    },
    {
      "epoch": 12.837837837837839,
      "grad_norm": 6.946944713592529,
      "learning_rate": 2.704847704847705e-05,
      "loss": 0.2931,
      "step": 6650
    },
    {
      "epoch": 12.886100386100386,
      "grad_norm": 7.03147554397583,
      "learning_rate": 2.6941226941226945e-05,
      "loss": 0.3579,
      "step": 6675
    },
    {
      "epoch": 12.934362934362934,
      "grad_norm": 5.406448841094971,
      "learning_rate": 2.6833976833976838e-05,
      "loss": 0.3175,
      "step": 6700
    },
    {
      "epoch": 12.982625482625483,
      "grad_norm": 8.631007194519043,
      "learning_rate": 2.672672672672673e-05,
      "loss": 0.2914,
      "step": 6725
    },
    {
      "epoch": 13.0,
      "eval_accuracy": 0.8403770142900577,
      "eval_f1_weighted": 0.8378115307589061,
      "eval_loss": 0.5945734977722168,
      "eval_runtime": 36.4396,
      "eval_samples_per_second": 90.259,
      "eval_steps_per_second": 0.96,
      "step": 6734
    },
    {
      "epoch": 13.03088803088803,
      "grad_norm": 5.884007930755615,
      "learning_rate": 2.6623766623766627e-05,
      "loss": 0.2917,
      "step": 6750
    },
    {
      "epoch": 13.079150579150578,
      "grad_norm": 7.237651348114014,
      "learning_rate": 2.6516516516516517e-05,
      "loss": 0.2258,
      "step": 6775
    },
    {
      "epoch": 13.127413127413128,
      "grad_norm": 7.724853038787842,
      "learning_rate": 2.640926640926641e-05,
      "loss": 0.2682,
      "step": 6800
    },
    {
      "epoch": 13.175675675675675,
      "grad_norm": 4.634833812713623,
      "learning_rate": 2.6302016302016303e-05,
      "loss": 0.2828,
      "step": 6825
    },
    {
      "epoch": 13.223938223938223,
      "grad_norm": 4.858809947967529,
      "learning_rate": 2.6194766194766196e-05,
      "loss": 0.2697,
      "step": 6850
    },
    {
      "epoch": 13.272200772200772,
      "grad_norm": 7.259544372558594,
      "learning_rate": 2.608751608751609e-05,
      "loss": 0.2818,
      "step": 6875
    },
    {
      "epoch": 13.32046332046332,
      "grad_norm": 4.365123271942139,
      "learning_rate": 2.5980265980265982e-05,
      "loss": 0.3458,
      "step": 6900
    },
    {
      "epoch": 13.36872586872587,
      "grad_norm": 5.144017219543457,
      "learning_rate": 2.5873015873015878e-05,
      "loss": 0.2628,
      "step": 6925
    },
    {
      "epoch": 13.416988416988417,
      "grad_norm": 4.878044605255127,
      "learning_rate": 2.5765765765765764e-05,
      "loss": 0.3484,
      "step": 6950
    },
    {
      "epoch": 13.465250965250965,
      "grad_norm": 4.477883815765381,
      "learning_rate": 2.5658515658515657e-05,
      "loss": 0.3027,
      "step": 6975
    },
    {
      "epoch": 13.513513513513514,
      "grad_norm": 6.384578704833984,
      "learning_rate": 2.555126555126555e-05,
      "loss": 0.2673,
      "step": 7000
    },
    {
      "epoch": 13.561776061776062,
      "grad_norm": 9.507566452026367,
      "learning_rate": 2.5444015444015447e-05,
      "loss": 0.3248,
      "step": 7025
    },
    {
      "epoch": 13.61003861003861,
      "grad_norm": 4.838878631591797,
      "learning_rate": 2.533676533676534e-05,
      "loss": 0.2746,
      "step": 7050
    },
    {
      "epoch": 13.658301158301159,
      "grad_norm": 7.897764682769775,
      "learning_rate": 2.5229515229515233e-05,
      "loss": 0.2832,
      "step": 7075
    },
    {
      "epoch": 13.706563706563706,
      "grad_norm": 4.214570999145508,
      "learning_rate": 2.5122265122265126e-05,
      "loss": 0.2672,
      "step": 7100
    },
    {
      "epoch": 13.754826254826256,
      "grad_norm": 8.185173988342285,
      "learning_rate": 2.5015015015015015e-05,
      "loss": 0.2397,
      "step": 7125
    },
    {
      "epoch": 13.803088803088803,
      "grad_norm": 8.73723030090332,
      "learning_rate": 2.490776490776491e-05,
      "loss": 0.2991,
      "step": 7150
    },
    {
      "epoch": 13.85135135135135,
      "grad_norm": 7.003711223602295,
      "learning_rate": 2.48005148005148e-05,
      "loss": 0.3089,
      "step": 7175
    },
    {
      "epoch": 13.8996138996139,
      "grad_norm": 5.633702754974365,
      "learning_rate": 2.4693264693264695e-05,
      "loss": 0.2697,
      "step": 7200
    },
    {
      "epoch": 13.947876447876448,
      "grad_norm": 17.90674591064453,
      "learning_rate": 2.4586014586014588e-05,
      "loss": 0.2799,
      "step": 7225
    },
    {
      "epoch": 13.996138996138995,
      "grad_norm": 9.889211654663086,
      "learning_rate": 2.447876447876448e-05,
      "loss": 0.2925,
      "step": 7250
    },
    {
      "epoch": 14.0,
      "eval_accuracy": 0.8458498023715415,
      "eval_f1_weighted": 0.8437738654017864,
      "eval_loss": 0.6123788356781006,
      "eval_runtime": 36.7889,
      "eval_samples_per_second": 89.402,
      "eval_steps_per_second": 0.951,
      "step": 7252
    },
    {
      "epoch": 14.044401544401545,
      "grad_norm": 11.969647407531738,
      "learning_rate": 2.4371514371514374e-05,
      "loss": 0.2785,
      "step": 7275
    },
    {
      "epoch": 14.092664092664092,
      "grad_norm": 7.726816654205322,
      "learning_rate": 2.4264264264264267e-05,
      "loss": 0.2636,
      "step": 7300
    },
    {
      "epoch": 14.14092664092664,
      "grad_norm": 10.18710708618164,
      "learning_rate": 2.4157014157014156e-05,
      "loss": 0.2891,
      "step": 7325
    },
    {
      "epoch": 14.18918918918919,
      "grad_norm": 10.101142883300781,
      "learning_rate": 2.404976404976405e-05,
      "loss": 0.3202,
      "step": 7350
    },
    {
      "epoch": 14.237451737451737,
      "grad_norm": 2.1645357608795166,
      "learning_rate": 2.3942513942513946e-05,
      "loss": 0.3027,
      "step": 7375
    },
    {
      "epoch": 14.285714285714286,
      "grad_norm": 13.201642990112305,
      "learning_rate": 2.3835263835263835e-05,
      "loss": 0.2626,
      "step": 7400
    },
    {
      "epoch": 14.333976833976834,
      "grad_norm": 9.166862487792969,
      "learning_rate": 2.3728013728013728e-05,
      "loss": 0.2448,
      "step": 7425
    },
    {
      "epoch": 14.382239382239382,
      "grad_norm": 4.289238452911377,
      "learning_rate": 2.362076362076362e-05,
      "loss": 0.2617,
      "step": 7450
    },
    {
      "epoch": 14.430501930501931,
      "grad_norm": 5.053163528442383,
      "learning_rate": 2.3513513513513518e-05,
      "loss": 0.2558,
      "step": 7475
    },
    {
      "epoch": 14.478764478764479,
      "grad_norm": 3.9468703269958496,
      "learning_rate": 2.3406263406263407e-05,
      "loss": 0.2498,
      "step": 7500
    },
    {
      "epoch": 14.527027027027026,
      "grad_norm": 11.210172653198242,
      "learning_rate": 2.32990132990133e-05,
      "loss": 0.265,
      "step": 7525
    },
    {
      "epoch": 14.575289575289576,
      "grad_norm": 6.643344402313232,
      "learning_rate": 2.3191763191763193e-05,
      "loss": 0.2371,
      "step": 7550
    },
    {
      "epoch": 14.623552123552123,
      "grad_norm": 6.674232006072998,
      "learning_rate": 2.3084513084513086e-05,
      "loss": 0.2911,
      "step": 7575
    },
    {
      "epoch": 14.671814671814673,
      "grad_norm": 7.760565757751465,
      "learning_rate": 2.297726297726298e-05,
      "loss": 0.2605,
      "step": 7600
    },
    {
      "epoch": 14.72007722007722,
      "grad_norm": 8.237442970275879,
      "learning_rate": 2.2870012870012872e-05,
      "loss": 0.2822,
      "step": 7625
    },
    {
      "epoch": 14.768339768339768,
      "grad_norm": 5.3566389083862305,
      "learning_rate": 2.2762762762762762e-05,
      "loss": 0.2695,
      "step": 7650
    },
    {
      "epoch": 14.816602316602317,
      "grad_norm": 17.232698440551758,
      "learning_rate": 2.2655512655512655e-05,
      "loss": 0.2653,
      "step": 7675
    },
    {
      "epoch": 14.864864864864865,
      "grad_norm": 8.5807466506958,
      "learning_rate": 2.254826254826255e-05,
      "loss": 0.3231,
      "step": 7700
    },
    {
      "epoch": 14.913127413127413,
      "grad_norm": 15.426214218139648,
      "learning_rate": 2.2441012441012444e-05,
      "loss": 0.2656,
      "step": 7725
    },
    {
      "epoch": 14.961389961389962,
      "grad_norm": 7.914007186889648,
      "learning_rate": 2.2333762333762334e-05,
      "loss": 0.273,
      "step": 7750
    },
    {
      "epoch": 15.0,
      "eval_accuracy": 0.8376406202493158,
      "eval_f1_weighted": 0.8359698071053023,
      "eval_loss": 0.6348521709442139,
      "eval_runtime": 36.6299,
      "eval_samples_per_second": 89.79,
      "eval_steps_per_second": 0.956,
      "step": 7770
    },
    {
      "epoch": 15.00965250965251,
      "grad_norm": 5.109002113342285,
      "learning_rate": 2.2226512226512227e-05,
      "loss": 0.2731,
      "step": 7775
    },
    {
      "epoch": 15.057915057915057,
      "grad_norm": 7.443646430969238,
      "learning_rate": 2.211926211926212e-05,
      "loss": 0.2408,
      "step": 7800
    },
    {
      "epoch": 15.106177606177607,
      "grad_norm": 6.1329145431518555,
      "learning_rate": 2.2012012012012013e-05,
      "loss": 0.2465,
      "step": 7825
    },
    {
      "epoch": 15.154440154440154,
      "grad_norm": 7.174223899841309,
      "learning_rate": 2.1904761904761906e-05,
      "loss": 0.2677,
      "step": 7850
    },
    {
      "epoch": 15.202702702702704,
      "grad_norm": 7.103278636932373,
      "learning_rate": 2.17975117975118e-05,
      "loss": 0.26,
      "step": 7875
    },
    {
      "epoch": 15.250965250965251,
      "grad_norm": 8.060030937194824,
      "learning_rate": 2.169026169026169e-05,
      "loss": 0.2532,
      "step": 7900
    },
    {
      "epoch": 15.299227799227799,
      "grad_norm": 4.185541152954102,
      "learning_rate": 2.1583011583011585e-05,
      "loss": 0.237,
      "step": 7925
    },
    {
      "epoch": 15.347490347490348,
      "grad_norm": 13.335055351257324,
      "learning_rate": 2.1475761475761478e-05,
      "loss": 0.2956,
      "step": 7950
    },
    {
      "epoch": 15.395752895752896,
      "grad_norm": 10.429760932922363,
      "learning_rate": 2.136851136851137e-05,
      "loss": 0.2589,
      "step": 7975
    },
    {
      "epoch": 15.444015444015443,
      "grad_norm": 8.283280372619629,
      "learning_rate": 2.126126126126126e-05,
      "loss": 0.295,
      "step": 8000
    },
    {
      "epoch": 15.492277992277993,
      "grad_norm": 7.3989787101745605,
      "learning_rate": 2.1154011154011154e-05,
      "loss": 0.2247,
      "step": 8025
    },
    {
      "epoch": 15.54054054054054,
      "grad_norm": 8.337364196777344,
      "learning_rate": 2.104676104676105e-05,
      "loss": 0.2555,
      "step": 8050
    },
    {
      "epoch": 15.58880308880309,
      "grad_norm": 6.939760208129883,
      "learning_rate": 2.093951093951094e-05,
      "loss": 0.2419,
      "step": 8075
    },
    {
      "epoch": 15.637065637065637,
      "grad_norm": 3.9523587226867676,
      "learning_rate": 2.0832260832260833e-05,
      "loss": 0.2698,
      "step": 8100
    },
    {
      "epoch": 15.685328185328185,
      "grad_norm": 4.552426815032959,
      "learning_rate": 2.0725010725010726e-05,
      "loss": 0.2508,
      "step": 8125
    },
    {
      "epoch": 15.733590733590734,
      "grad_norm": 8.854517936706543,
      "learning_rate": 2.061776061776062e-05,
      "loss": 0.2952,
      "step": 8150
    },
    {
      "epoch": 15.781853281853282,
      "grad_norm": 6.381838321685791,
      "learning_rate": 2.051051051051051e-05,
      "loss": 0.2533,
      "step": 8175
    },
    {
      "epoch": 15.83011583011583,
      "grad_norm": 7.330158233642578,
      "learning_rate": 2.0403260403260405e-05,
      "loss": 0.2492,
      "step": 8200
    },
    {
      "epoch": 15.878378378378379,
      "grad_norm": 20.540800094604492,
      "learning_rate": 2.0296010296010298e-05,
      "loss": 0.282,
      "step": 8225
    },
    {
      "epoch": 15.926640926640927,
      "grad_norm": 8.125656127929688,
      "learning_rate": 2.018876018876019e-05,
      "loss": 0.2422,
      "step": 8250
    },
    {
      "epoch": 15.974903474903474,
      "grad_norm": 2.0944581031799316,
      "learning_rate": 2.0081510081510084e-05,
      "loss": 0.2365,
      "step": 8275
    },
    {
      "epoch": 16.0,
      "eval_accuracy": 0.8385527515962299,
      "eval_f1_weighted": 0.837313078383669,
      "eval_loss": 0.6917495131492615,
      "eval_runtime": 36.5233,
      "eval_samples_per_second": 90.052,
      "eval_steps_per_second": 0.958,
      "step": 8288
    },
    {
      "epoch": 16.023166023166024,
      "grad_norm": 7.274285316467285,
      "learning_rate": 1.9974259974259977e-05,
      "loss": 0.2871,
      "step": 8300
    },
    {
      "epoch": 16.071428571428573,
      "grad_norm": 3.8650145530700684,
      "learning_rate": 1.9867009867009866e-05,
      "loss": 0.2303,
      "step": 8325
    },
    {
      "epoch": 16.11969111969112,
      "grad_norm": 5.414699554443359,
      "learning_rate": 1.975975975975976e-05,
      "loss": 0.2162,
      "step": 8350
    },
    {
      "epoch": 16.167953667953668,
      "grad_norm": 4.987058162689209,
      "learning_rate": 1.9652509652509656e-05,
      "loss": 0.2256,
      "step": 8375
    },
    {
      "epoch": 16.216216216216218,
      "grad_norm": 10.9288969039917,
      "learning_rate": 1.9545259545259545e-05,
      "loss": 0.2727,
      "step": 8400
    },
    {
      "epoch": 16.264478764478763,
      "grad_norm": 3.4578616619110107,
      "learning_rate": 1.9438009438009438e-05,
      "loss": 0.2236,
      "step": 8425
    },
    {
      "epoch": 16.312741312741313,
      "grad_norm": 9.90953254699707,
      "learning_rate": 1.933075933075933e-05,
      "loss": 0.2539,
      "step": 8450
    },
    {
      "epoch": 16.361003861003862,
      "grad_norm": 8.125014305114746,
      "learning_rate": 1.9223509223509224e-05,
      "loss": 0.2855,
      "step": 8475
    },
    {
      "epoch": 16.409266409266408,
      "grad_norm": 12.736306190490723,
      "learning_rate": 1.9116259116259117e-05,
      "loss": 0.277,
      "step": 8500
    },
    {
      "epoch": 16.457528957528957,
      "grad_norm": 11.00949478149414,
      "learning_rate": 1.900900900900901e-05,
      "loss": 0.2707,
      "step": 8525
    },
    {
      "epoch": 16.505791505791507,
      "grad_norm": 3.8494718074798584,
      "learning_rate": 1.8901758901758903e-05,
      "loss": 0.2461,
      "step": 8550
    },
    {
      "epoch": 16.554054054054053,
      "grad_norm": 10.802850723266602,
      "learning_rate": 1.8794508794508793e-05,
      "loss": 0.2461,
      "step": 8575
    },
    {
      "epoch": 16.602316602316602,
      "grad_norm": 5.478791236877441,
      "learning_rate": 1.868725868725869e-05,
      "loss": 0.2527,
      "step": 8600
    },
    {
      "epoch": 16.65057915057915,
      "grad_norm": 11.62106990814209,
      "learning_rate": 1.8580008580008582e-05,
      "loss": 0.2675,
      "step": 8625
    },
    {
      "epoch": 16.698841698841697,
      "grad_norm": 6.278982639312744,
      "learning_rate": 1.8472758472758472e-05,
      "loss": 0.233,
      "step": 8650
    },
    {
      "epoch": 16.747104247104247,
      "grad_norm": 3.6616737842559814,
      "learning_rate": 1.8365508365508365e-05,
      "loss": 0.2286,
      "step": 8675
    },
    {
      "epoch": 16.795366795366796,
      "grad_norm": 4.318908214569092,
      "learning_rate": 1.8258258258258258e-05,
      "loss": 0.2133,
      "step": 8700
    },
    {
      "epoch": 16.843629343629345,
      "grad_norm": 9.866072654724121,
      "learning_rate": 1.8151008151008154e-05,
      "loss": 0.2368,
      "step": 8725
    },
    {
      "epoch": 16.89189189189189,
      "grad_norm": 15.483420372009277,
      "learning_rate": 1.8043758043758044e-05,
      "loss": 0.2136,
      "step": 8750
    },
    {
      "epoch": 16.94015444015444,
      "grad_norm": 7.322241306304932,
      "learning_rate": 1.7936507936507937e-05,
      "loss": 0.2671,
      "step": 8775
    },
    {
      "epoch": 16.98841698841699,
      "grad_norm": 7.209441184997559,
      "learning_rate": 1.782925782925783e-05,
      "loss": 0.2121,
      "step": 8800
    },
    {
      "epoch": 17.0,
      "eval_accuracy": 0.8470659775007601,
      "eval_f1_weighted": 0.8449245363731323,
      "eval_loss": 0.6752339601516724,
      "eval_runtime": 36.6373,
      "eval_samples_per_second": 89.772,
      "eval_steps_per_second": 0.955,
      "step": 8806
    },
    {
      "epoch": 17.036679536679536,
      "grad_norm": 6.582106113433838,
      "learning_rate": 1.7722007722007723e-05,
      "loss": 0.2416,
      "step": 8825
    },
    {
      "epoch": 17.084942084942085,
      "grad_norm": 6.28384256362915,
      "learning_rate": 1.761904761904762e-05,
      "loss": 0.2636,
      "step": 8850
    },
    {
      "epoch": 17.133204633204635,
      "grad_norm": 6.615358829498291,
      "learning_rate": 1.7511797511797513e-05,
      "loss": 0.2057,
      "step": 8875
    },
    {
      "epoch": 17.18146718146718,
      "grad_norm": 6.497164726257324,
      "learning_rate": 1.7404547404547406e-05,
      "loss": 0.2199,
      "step": 8900
    },
    {
      "epoch": 17.22972972972973,
      "grad_norm": 7.25261926651001,
      "learning_rate": 1.72972972972973e-05,
      "loss": 0.2517,
      "step": 8925
    },
    {
      "epoch": 17.27799227799228,
      "grad_norm": 4.229773044586182,
      "learning_rate": 1.7190047190047188e-05,
      "loss": 0.2293,
      "step": 8950
    },
    {
      "epoch": 17.326254826254825,
      "grad_norm": 8.245392799377441,
      "learning_rate": 1.7082797082797085e-05,
      "loss": 0.2188,
      "step": 8975
    },
    {
      "epoch": 17.374517374517374,
      "grad_norm": 12.843172073364258,
      "learning_rate": 1.6975546975546978e-05,
      "loss": 0.26,
      "step": 9000
    },
    {
      "epoch": 17.422779922779924,
      "grad_norm": 8.627662658691406,
      "learning_rate": 1.6868296868296867e-05,
      "loss": 0.2184,
      "step": 9025
    },
    {
      "epoch": 17.47104247104247,
      "grad_norm": 4.213763236999512,
      "learning_rate": 1.676104676104676e-05,
      "loss": 0.2404,
      "step": 9050
    },
    {
      "epoch": 17.51930501930502,
      "grad_norm": 6.329771518707275,
      "learning_rate": 1.6653796653796657e-05,
      "loss": 0.2048,
      "step": 9075
    },
    {
      "epoch": 17.56756756756757,
      "grad_norm": 5.60711145401001,
      "learning_rate": 1.654654654654655e-05,
      "loss": 0.2372,
      "step": 9100
    },
    {
      "epoch": 17.615830115830114,
      "grad_norm": 3.9121930599212646,
      "learning_rate": 1.643929643929644e-05,
      "loss": 0.2432,
      "step": 9125
    },
    {
      "epoch": 17.664092664092664,
      "grad_norm": 5.098020076751709,
      "learning_rate": 1.6332046332046332e-05,
      "loss": 0.2515,
      "step": 9150
    },
    {
      "epoch": 17.712355212355213,
      "grad_norm": 8.323753356933594,
      "learning_rate": 1.6224796224796225e-05,
      "loss": 0.2373,
      "step": 9175
    },
    {
      "epoch": 17.760617760617762,
      "grad_norm": 6.194894790649414,
      "learning_rate": 1.6117546117546118e-05,
      "loss": 0.3013,
      "step": 9200
    },
    {
      "epoch": 17.80888030888031,
      "grad_norm": 6.9089436531066895,
      "learning_rate": 1.601029601029601e-05,
      "loss": 0.2443,
      "step": 9225
    },
    {
      "epoch": 17.857142857142858,
      "grad_norm": 6.956424713134766,
      "learning_rate": 1.5903045903045904e-05,
      "loss": 0.2429,
      "step": 9250
    },
    {
      "epoch": 17.905405405405407,
      "grad_norm": 6.147084712982178,
      "learning_rate": 1.5795795795795794e-05,
      "loss": 0.229,
      "step": 9275
    },
    {
      "epoch": 17.953667953667953,
      "grad_norm": 5.644355773925781,
      "learning_rate": 1.568854568854569e-05,
      "loss": 0.2583,
      "step": 9300
    },
    {
      "epoch": 18.0,
      "eval_accuracy": 0.8461538461538461,
      "eval_f1_weighted": 0.8443643547259521,
      "eval_loss": 0.7191227674484253,
      "eval_runtime": 36.4854,
      "eval_samples_per_second": 90.146,
      "eval_steps_per_second": 0.959,
      "step": 9324
    },
    {
      "epoch": 18.001930501930502,
      "grad_norm": 5.36644983291626,
      "learning_rate": 1.5581295581295583e-05,
      "loss": 0.2277,
      "step": 9325
    },
    {
      "epoch": 18.05019305019305,
      "grad_norm": 4.884340763092041,
      "learning_rate": 1.5474045474045476e-05,
      "loss": 0.2015,
      "step": 9350
    },
    {
      "epoch": 18.098455598455597,
      "grad_norm": 8.023367881774902,
      "learning_rate": 1.5366795366795366e-05,
      "loss": 0.2856,
      "step": 9375
    },
    {
      "epoch": 18.146718146718147,
      "grad_norm": 4.084275722503662,
      "learning_rate": 1.525954525954526e-05,
      "loss": 0.2312,
      "step": 9400
    },
    {
      "epoch": 18.194980694980696,
      "grad_norm": 5.301934719085693,
      "learning_rate": 1.5152295152295154e-05,
      "loss": 0.2023,
      "step": 9425
    },
    {
      "epoch": 18.243243243243242,
      "grad_norm": 2.191399574279785,
      "learning_rate": 1.5045045045045045e-05,
      "loss": 0.2368,
      "step": 9450
    },
    {
      "epoch": 18.29150579150579,
      "grad_norm": 11.585858345031738,
      "learning_rate": 1.4937794937794938e-05,
      "loss": 0.199,
      "step": 9475
    },
    {
      "epoch": 18.33976833976834,
      "grad_norm": 2.771127462387085,
      "learning_rate": 1.4830544830544833e-05,
      "loss": 0.2225,
      "step": 9500
    },
    {
      "epoch": 18.388030888030887,
      "grad_norm": 9.850549697875977,
      "learning_rate": 1.4723294723294722e-05,
      "loss": 0.206,
      "step": 9525
    },
    {
      "epoch": 18.436293436293436,
      "grad_norm": 7.40004825592041,
      "learning_rate": 1.4616044616044617e-05,
      "loss": 0.2387,
      "step": 9550
    },
    {
      "epoch": 18.484555984555985,
      "grad_norm": 4.896059989929199,
      "learning_rate": 1.450879450879451e-05,
      "loss": 0.2092,
      "step": 9575
    },
    {
      "epoch": 18.53281853281853,
      "grad_norm": 11.431527137756348,
      "learning_rate": 1.4401544401544403e-05,
      "loss": 0.2131,
      "step": 9600
    },
    {
      "epoch": 18.58108108108108,
      "grad_norm": 10.647073745727539,
      "learning_rate": 1.4294294294294294e-05,
      "loss": 0.2381,
      "step": 9625
    },
    {
      "epoch": 18.62934362934363,
      "grad_norm": 6.204046726226807,
      "learning_rate": 1.4187044187044187e-05,
      "loss": 0.1813,
      "step": 9650
    },
    {
      "epoch": 18.677606177606176,
      "grad_norm": 11.955113410949707,
      "learning_rate": 1.4079794079794082e-05,
      "loss": 0.2251,
      "step": 9675
    },
    {
      "epoch": 18.725868725868725,
      "grad_norm": 5.207870960235596,
      "learning_rate": 1.3972543972543972e-05,
      "loss": 0.2613,
      "step": 9700
    },
    {
      "epoch": 18.774131274131275,
      "grad_norm": 6.897366046905518,
      "learning_rate": 1.3865293865293866e-05,
      "loss": 0.2285,
      "step": 9725
    },
    {
      "epoch": 18.822393822393824,
      "grad_norm": 4.229111671447754,
      "learning_rate": 1.375804375804376e-05,
      "loss": 0.2261,
      "step": 9750
    },
    {
      "epoch": 18.87065637065637,
      "grad_norm": 6.513268947601318,
      "learning_rate": 1.365079365079365e-05,
      "loss": 0.2326,
      "step": 9775
    },
    {
      "epoch": 18.91891891891892,
      "grad_norm": 6.453638553619385,
      "learning_rate": 1.3543543543543544e-05,
      "loss": 0.2376,
      "step": 9800
    },
    {
      "epoch": 18.96718146718147,
      "grad_norm": 2.90339732170105,
      "learning_rate": 1.3436293436293437e-05,
      "loss": 0.2032,
      "step": 9825
    },
    {
      "epoch": 19.0,
      "eval_accuracy": 0.8461538461538461,
      "eval_f1_weighted": 0.8442744023211336,
      "eval_loss": 0.7726643085479736,
      "eval_runtime": 36.6886,
      "eval_samples_per_second": 89.646,
      "eval_steps_per_second": 0.954,
      "step": 9842
    },
    {
      "epoch": 19.015444015444015,
      "grad_norm": 6.532814979553223,
      "learning_rate": 1.3329043329043331e-05,
      "loss": 0.2431,
      "step": 9850
    },
    {
      "epoch": 19.063706563706564,
      "grad_norm": 11.60678768157959,
      "learning_rate": 1.3226083226083228e-05,
      "loss": 0.2005,
      "step": 9875
    },
    {
      "epoch": 19.111969111969113,
      "grad_norm": 4.87822151184082,
      "learning_rate": 1.3118833118833118e-05,
      "loss": 0.2084,
      "step": 9900
    },
    {
      "epoch": 19.16023166023166,
      "grad_norm": 4.458703517913818,
      "learning_rate": 1.3011583011583012e-05,
      "loss": 0.2285,
      "step": 9925
    },
    {
      "epoch": 19.20849420849421,
      "grad_norm": 5.315597057342529,
      "learning_rate": 1.2904332904332905e-05,
      "loss": 0.2229,
      "step": 9950
    },
    {
      "epoch": 19.256756756756758,
      "grad_norm": 3.2609517574310303,
      "learning_rate": 1.2797082797082798e-05,
      "loss": 0.2183,
      "step": 9975
    },
    {
      "epoch": 19.305019305019304,
      "grad_norm": 16.7747802734375,
      "learning_rate": 1.268983268983269e-05,
      "loss": 0.2393,
      "step": 10000
    },
    {
      "epoch": 19.353281853281853,
      "grad_norm": 7.115992069244385,
      "learning_rate": 1.2582582582582583e-05,
      "loss": 0.2266,
      "step": 10025
    },
    {
      "epoch": 19.401544401544403,
      "grad_norm": 0.9544990062713623,
      "learning_rate": 1.2475332475332476e-05,
      "loss": 0.2147,
      "step": 10050
    },
    {
      "epoch": 19.44980694980695,
      "grad_norm": 7.356318473815918,
      "learning_rate": 1.2368082368082369e-05,
      "loss": 0.2138,
      "step": 10075
    },
    {
      "epoch": 19.498069498069498,
      "grad_norm": 5.945486545562744,
      "learning_rate": 1.2260832260832262e-05,
      "loss": 0.2264,
      "step": 10100
    },
    {
      "epoch": 19.546332046332047,
      "grad_norm": 4.5792694091796875,
      "learning_rate": 1.2153582153582155e-05,
      "loss": 0.2472,
      "step": 10125
    },
    {
      "epoch": 19.594594594594593,
      "grad_norm": 5.6434478759765625,
      "learning_rate": 1.2046332046332048e-05,
      "loss": 0.2296,
      "step": 10150
    },
    {
      "epoch": 19.642857142857142,
      "grad_norm": 3.274287223815918,
      "learning_rate": 1.1939081939081939e-05,
      "loss": 0.2318,
      "step": 10175
    },
    {
      "epoch": 19.69111969111969,
      "grad_norm": 7.4295148849487305,
      "learning_rate": 1.1831831831831832e-05,
      "loss": 0.2082,
      "step": 10200
    },
    {
      "epoch": 19.739382239382238,
      "grad_norm": 23.2170467376709,
      "learning_rate": 1.1724581724581725e-05,
      "loss": 0.1959,
      "step": 10225
    },
    {
      "epoch": 19.787644787644787,
      "grad_norm": 5.3595757484436035,
      "learning_rate": 1.1617331617331618e-05,
      "loss": 0.2293,
      "step": 10250
    },
    {
      "epoch": 19.835907335907336,
      "grad_norm": 4.907078266143799,
      "learning_rate": 1.1510081510081511e-05,
      "loss": 0.2175,
      "step": 10275
    },
    {
      "epoch": 19.884169884169886,
      "grad_norm": 4.855776309967041,
      "learning_rate": 1.1402831402831402e-05,
      "loss": 0.2157,
      "step": 10300
    },
    {
      "epoch": 19.93243243243243,
      "grad_norm": 2.940319299697876,
      "learning_rate": 1.1295581295581297e-05,
      "loss": 0.2053,
      "step": 10325
    },
    {
      "epoch": 19.98069498069498,
      "grad_norm": 5.828269004821777,
      "learning_rate": 1.1188331188331188e-05,
      "loss": 0.1958,
      "step": 10350
    },
    {
      "epoch": 20.0,
      "eval_accuracy": 0.8455457585892369,
      "eval_f1_weighted": 0.8433248658704725,
      "eval_loss": 0.856026828289032,
      "eval_runtime": 36.2884,
      "eval_samples_per_second": 90.635,
      "eval_steps_per_second": 0.964,
      "step": 10360
    },
    {
      "epoch": 20.02895752895753,
      "grad_norm": 2.0008115768432617,
      "learning_rate": 1.1081081081081083e-05,
      "loss": 0.1829,
      "step": 10375
    },
    {
      "epoch": 20.077220077220076,
      "grad_norm": 9.503396034240723,
      "learning_rate": 1.0973830973830974e-05,
      "loss": 0.2147,
      "step": 10400
    },
    {
      "epoch": 20.125482625482626,
      "grad_norm": 4.459254264831543,
      "learning_rate": 1.0866580866580867e-05,
      "loss": 0.2115,
      "step": 10425
    },
    {
      "epoch": 20.173745173745175,
      "grad_norm": 15.986419677734375,
      "learning_rate": 1.075933075933076e-05,
      "loss": 0.1925,
      "step": 10450
    },
    {
      "epoch": 20.22200772200772,
      "grad_norm": 5.218672752380371,
      "learning_rate": 1.0652080652080652e-05,
      "loss": 0.2122,
      "step": 10475
    },
    {
      "epoch": 20.27027027027027,
      "grad_norm": 2.6996800899505615,
      "learning_rate": 1.0544830544830546e-05,
      "loss": 0.2013,
      "step": 10500
    },
    {
      "epoch": 20.31853281853282,
      "grad_norm": 4.7007246017456055,
      "learning_rate": 1.0437580437580438e-05,
      "loss": 0.2015,
      "step": 10525
    },
    {
      "epoch": 20.366795366795365,
      "grad_norm": 5.066750526428223,
      "learning_rate": 1.033033033033033e-05,
      "loss": 0.2262,
      "step": 10550
    },
    {
      "epoch": 20.415057915057915,
      "grad_norm": 4.435134410858154,
      "learning_rate": 1.0223080223080224e-05,
      "loss": 0.1986,
      "step": 10575
    },
    {
      "epoch": 20.463320463320464,
      "grad_norm": 4.884032249450684,
      "learning_rate": 1.0115830115830117e-05,
      "loss": 0.1845,
      "step": 10600
    },
    {
      "epoch": 20.51158301158301,
      "grad_norm": 11.860152244567871,
      "learning_rate": 1.000858000858001e-05,
      "loss": 0.1885,
      "step": 10625
    },
    {
      "epoch": 20.55984555984556,
      "grad_norm": 7.246563911437988,
      "learning_rate": 9.901329901329901e-06,
      "loss": 0.1955,
      "step": 10650
    },
    {
      "epoch": 20.60810810810811,
      "grad_norm": 4.302550792694092,
      "learning_rate": 9.794079794079794e-06,
      "loss": 0.2218,
      "step": 10675
    },
    {
      "epoch": 20.656370656370655,
      "grad_norm": 18.953035354614258,
      "learning_rate": 9.686829686829687e-06,
      "loss": 0.1914,
      "step": 10700
    },
    {
      "epoch": 20.704633204633204,
      "grad_norm": 2.8861351013183594,
      "learning_rate": 9.57957957957958e-06,
      "loss": 0.2099,
      "step": 10725
    },
    {
      "epoch": 20.752895752895753,
      "grad_norm": 15.029097557067871,
      "learning_rate": 9.472329472329473e-06,
      "loss": 0.1951,
      "step": 10750
    },
    {
      "epoch": 20.801158301158303,
      "grad_norm": 6.430077075958252,
      "learning_rate": 9.365079365079366e-06,
      "loss": 0.2222,
      "step": 10775
    },
    {
      "epoch": 20.84942084942085,
      "grad_norm": 17.76978874206543,
      "learning_rate": 9.257829257829257e-06,
      "loss": 0.2119,
      "step": 10800
    },
    {
      "epoch": 20.897683397683398,
      "grad_norm": 11.569830894470215,
      "learning_rate": 9.150579150579152e-06,
      "loss": 0.1971,
      "step": 10825
    },
    {
      "epoch": 20.945945945945947,
      "grad_norm": 4.2353129386901855,
      "learning_rate": 9.043329043329043e-06,
      "loss": 0.191,
      "step": 10850
    },
    {
      "epoch": 20.994208494208493,
      "grad_norm": 17.180517196655273,
      "learning_rate": 8.936078936078936e-06,
      "loss": 0.2227,
      "step": 10875
    },
    {
      "epoch": 21.0,
      "eval_accuracy": 0.8434174521131043,
      "eval_f1_weighted": 0.8415611282820631,
      "eval_loss": 0.8403311967849731,
      "eval_runtime": 36.4015,
      "eval_samples_per_second": 90.353,
      "eval_steps_per_second": 0.961,
      "step": 10878
    }
  ],
  "logging_steps": 25,
  "max_steps": 12950,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 25,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.014379896614892e+19,
  "train_batch_size": 12,
  "trial_name": null,
  "trial_params": null
}

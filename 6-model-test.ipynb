{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b858227-0d3a-4502-a716-bf4260d52c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names=[\n",
    "    \"microsoft/swinv2-tiny-patch4-window16-256\",\n",
    "    \"microsoft/swinv2-base-patch4-window16-256\",\n",
    "    \"facebook/dinov2-base\",\n",
    "    \"nvidia/MambaVision-B-1K\",\n",
    "    \"microsoft/beit-base-patch16-224\",\n",
    "    \"google/vit-base-patch16-384\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08032e1e-4411-456b-a17e-6d43d67b72dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5757ed98-34f5-4d97-9b16-cb899bce49a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f38061a-6481-4689-b30e-119d0dda1b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "396a8295-7ba5-402b-b317-c67aca19250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "951548d2-2e94-40f4-b928-ee8c20ad55b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d82b26a-bfd8-4d6b-be06-10b378a0c809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dcc4007-d68b-4d79-b3b0-b9ac750fc92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, PretrainedConfig, AutoConfig\n",
    "class ImageMultiRegressionConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_size=3,\n",
    "        init_checkpoint=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.output_size=output_size\n",
    "        self.init_checkpoint=init_checkpoint\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "class ImageMultiRegressionModel(PreTrainedModel):\n",
    "    config_class=ImageMultiRegressionConfig\n",
    "    def __init__(self, config, loss=nn.MSELoss()):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.inner_model = AutoModel.from_pretrained(config.init_checkpoint)\n",
    "        self.classifier = nn.Linear(self.inner_model.config.hidden_size, config.output_size)\n",
    "        self.loss=loss\n",
    "    \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        outputs = self.inner_model(pixel_values=pixel_values)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # image embedding\n",
    "        values = self.classifier(cls_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss(values.view(-1), labels.view(-1))\n",
    "        return (loss, values) if loss is not None else values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c4c7c9d-75fa-41a4-8d8f-c7252961d5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset=load_from_disk(\"./data/dataset/\")\n",
    "dataset[\"train\"].set_format(\"torch\")\n",
    "dataset[\"test\"].set_format(\"torch\")\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torch\n",
    "\n",
    "_train_transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),  # Normalize expects float input\n",
    "    transforms.RandomRotation(degrees=20),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop(size=(256,256), scale=(.6,1.0), antialias=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def train_transform(ex):\n",
    "    if \"image\" in ex:\n",
    "        ex[\"pixel_values\"]=[_train_transform(image) for image in ex[\"image\"]]\n",
    "    return ex\n",
    "    \n",
    "_test_transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),  # Normalize expects float input\n",
    "    transforms.Resize(size=(256,256)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def test_transform(ex):\n",
    "    if \"image\" in ex:\n",
    "        ex[\"pixel_values\"]=[_test_transform(image) for image in ex[\"image\"]]\n",
    "    return ex\n",
    "\n",
    "#_columns=[\"pixel_values\", \"light_level\", \"fume_strength\", \"explosion_strength\"]\n",
    "dataset[\"train\"].set_transform(train_transform)\n",
    "dataset[\"test\"].set_transform(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b2301-e488-44ee-aa78-bde545a72e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d71e818-0554-4738-9744-034534c894b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7a1453-aab3-4449-ae49-a6848d38b608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61dd0f5f-40ca-4cf2-b97f-f7a861e4f443",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=torch.tensor(dataset[\"train\"][\"light_level\"])\n",
    "t2=torch.tensor(dataset[\"train\"][\"fume_strength\"])\n",
    "t3=torch.tensor(dataset[\"train\"][\"explosion_strength\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95c9c912-e692-4b3a-ae87-d431d795258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1=torch.tensor(dataset[\"test\"][\"light_level\"])\n",
    "y2=torch.tensor(dataset[\"test\"][\"fume_strength\"])\n",
    "y3=torch.tensor(dataset[\"test\"][\"explosion_strength\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b02a922-8e93-48f0-8197-a20bac96842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(x,y):\n",
    "    return {\"MSE\":sklearn.metrics.mean_squared_error(x, y),\n",
    "           \"MAE\":sklearn.metrics.mean_absolute_error(x,y),\n",
    "           \"R2_test\":sklearn.metrics.r2_score(x,y)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c84bb5a1-619f-4d74-b2cd-0238549bd5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stats.stackexchange.com/questions/228540/how-to-calculate-out-of-sample-r-squared/492581#492581\n",
    "# https://arxiv.org/pdf/2302.05131\n",
    "def oosR(MST, MSE): \n",
    "    return 1-MSE/MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc2854e1-0265-4474-a748-8ed176091783",
   "metadata": {},
   "outputs": [],
   "source": [
    "T=torch.stack([t1,t2,t3], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3428421e-a0d7-4f1b-96cf-69705e0c6486",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=torch.stack([y1,y2,y3], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68cd1721-8441-4915-869f-c66ecff95451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline: always predict mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b8e0df8-58e8-4d5c-ba43-c1c4b427d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_mean=T.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38fb3e3f-a445-4f10-b687-e16f33e614c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_mean_only = T_mean.repeat(Y.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db11fb7f-1d7f-4783-9deb-4babb0ec7d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1891, 3]), torch.Size([1891, 3]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_mean_only.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca369593-f7e8-42b1-944c-028bd5bfa1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9055, 0.7536, 0.3736])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_mean_only[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b9c5a4c-3f2b-4e0e-8afc-084678bf0ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSE': 0.1443669, 'MAE': 0.31176418, 'R2_test': 0.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_metrics = compute_metrics(T_mean_only, Y); mean_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "521d9fb9-92c4-40c2-b612-3d861d0dd0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline: always predict median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eecc39f7-7812-46a0-9e51-b1ca3f7590b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_median=T.median(dim=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3ac38e2-8b64-4a5e-b5f3-e2cfe4e8293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_median_only = T_median.repeat(Y.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3673ade7-6453-4309-9076-f589248f3033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1891, 3]), torch.Size([1891, 3]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_median_only.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c11252f9-de92-4389-886a-244a3bb69cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_median_only[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53c8a69d-1ee6-4375-aaec-2ca466c25505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSE': 0.20900238, 'MAE': 0.23368602, 'R2_test': 0.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_metrics = compute_metrics(T_median_only, Y); median_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64f78330-4c3a-4023-949d-458d3a4e039e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oosR(mean_metrics['MSE'], mean_metrics['MSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d41bba87-c6d0-4b99-982a-7d3a8993d094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.44771671295166016"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oosR(mean_metrics['MSE'], median_metrics['MSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af55b7a9-95b4-411c-9f18-c9172ce7e2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "deb04d11-22ce-4f14-9682-89656bb6aba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6337380793057688"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oosR(mean_metrics['MSE'], 0.052876099944114685) #swinv2-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "043ce240-cb71-43fc-8f00-452f416c0d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4848992067647788"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oosR(mean_metrics['MSE'], 0.07436350733041763) #dinov2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ed2b26c4-c895-4148-ae14-3f5ec9733ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6411147096100152"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oosR(mean_metrics['MSE'], 0.051811158657073975) #beit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c57ad7c-2ba9-4c2d-b699-b2db8da39dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44797ef-f461-4efc-8ef8-6dd9800e6328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "14d49dcf-9e89-4e58-97c1-8b358d2d1409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_to_class(x): #[b, 3]\n",
    "    t=np.full(x.shape[0], \"INA\")\n",
    "    t_f=np.full(x.shape[0], \"FUM\")\n",
    "    t_e=np.full(x.shape[0], \"EXP\")\n",
    "    t_ef=np.full(x.shape[0], \"EXP+FUM\")\n",
    "    t=np.where((x[:,1]>.75) & (x[:,2]>.1),t_ef,t)\n",
    "    t=np.where((x[:,1]>.75) & (x[:,2]<=.1),t_f,t)\n",
    "    t=np.where((x[:,1]<=.75) & (x[:,2]>.1),t_e,t)\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a706cb56-e1f9-4186-ab14-35c4303a0898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_Y=reg_to_class(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "03b11665-95fc-4c0c-ac8a-be13d8829040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['EXP', 'EXP+FUM', 'FUM', 'INA'], dtype='<U7'),\n",
       " array([ 460, 6358, 5821, 4378]))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(class_Y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c59c4b4-147b-4300-b478-e8dd2b8fcf1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

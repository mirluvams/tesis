{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b858227-0d3a-4502-a716-bf4260d52c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names=[\n",
    "    \"microsoft/swinv2-tiny-patch4-window16-256\",\n",
    "    \"facebook/dinov2-base\",\n",
    "    \"nvidia/MambaVision-B-1K\",\n",
    "    \"microsoft/beit-base-patch16-224\",\n",
    "    \"microsoft/swinv2-base-patch4-window16-256\",\n",
    "    \"google/vit-base-patch16-384\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08032e1e-4411-456b-a17e-6d43d67b72dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5757ed98-34f5-4d97-9b16-cb899bce49a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f38061a-6481-4689-b30e-119d0dda1b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eec179d8-dc2d-4c7c-849a-47ea0ee249e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "class ImageMultiRegressionModel(nn.Module):\n",
    "    def __init__(self, model, loss=nn.MSELoss(), output_size=1):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.classifier = nn.Linear(self.model.config.hidden_size, output_size)\n",
    "        self.loss=loss\n",
    "    \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        outputs = self.model(pixel_values=pixel_values)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # image embedding\n",
    "        values = self.classifier(cls_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss(values.view(-1), labels.view(-1))\n",
    "        return (loss, values) if loss is not None else values\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "396a8295-7ba5-402b-b317-c67aca19250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "951548d2-2e94-40f4-b928-ee8c20ad55b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3860a5e5-304f-4786-86c9-400d7c90d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model=AutoModel.from_pretrained(model_names[0])\n",
    "model=ImageMultiRegressionModel(raw_model, output_size=3).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f312b03f-8861-48d2-b8a1-969b7f2571fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor=AutoImageProcessor.from_pretrained(model_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "242735e2-24cb-4e00-b8a3-12b35e138857",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=image_processor(Image.open(\"./images/2024/image_2.jpg\"), return_tensors=\"pt\")[\"pixel_values\"].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6007b35-6ff3-43d1-8675-06d54a9701c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=model(inputs, labels=torch.tensor([[0,0,0]]).to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7a65049-3f88-4eb0-a3c6-ef9a8d179d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward0>),\n",
       " tensor([[ 0.0620,  0.0718, -0.0760]], device='cuda:0',\n",
       "        grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e9677-d22d-4bfc-a042-9633a28ac695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b941f321-4b53-43ed-afbc-12b3cbd63e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c4c7c9d-75fa-41a4-8d8f-c7252961d5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset=load_from_disk(\"./data/dataset/\")\n",
    "dataset[\"train\"].set_format(\"torch\")\n",
    "dataset[\"test\"].set_format(\"torch\")\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torch\n",
    "\n",
    "_train_transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),  # Normalize expects float input\n",
    "    transforms.RandomRotation(degrees=20),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop(size=(256,256), scale=(.6,1.0), antialias=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def train_transform(ex):\n",
    "    ex[\"pixel_values\"]=[_train_transform(image) for image in ex[\"image\"]]\n",
    "    return ex\n",
    "    \n",
    "_test_transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),  # Normalize expects float input\n",
    "    transforms.Resize(size=(256,256)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def test_transform(ex):\n",
    "    ex[\"pixel_values\"]=[_test_transform(image) for image in ex[\"image\"]]\n",
    "    return ex\n",
    "\n",
    "#_columns=[\"pixel_values\", \"light_level\", \"fume_strength\", \"explosion_strength\"]\n",
    "dataset[\"train\"].set_transform(train_transform)\n",
    "dataset[\"test\"].set_transform(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b2301-e488-44ee-aa78-bde545a72e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84055b3-df75-42b8-98f6-3e34b5e5a24a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75887b88-fbac-40a4-ad75-aacb008ed968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ee784ab-95e3-4633-b3a7-9730922c8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NCCL_P2P_DISABLE\"]=\"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"]=\"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6fafc44-de2e-4b7e-bdc6-586c10c9f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_args = TrainingArguments(\n",
    "    f\"~/tmp/model-training/modelname\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    #learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=12,\n",
    "    #gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=25,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=25,\n",
    "    load_best_model_at_end=True,\n",
    "    remove_unused_columns=False,\n",
    "    #metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    "    report_to=[]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde35578-be79-4c06-8bec-38ce9badc5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4f410f7-3b49-455a-9571-08e2bc6c43f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_columns=[\"light_level\",\"fume_strength\",\"explosion_strength\"]\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])#.to(DEVICE)\n",
    "    labels =  torch.stack([torch.tensor([example[c] for c in _columns]) for example in examples])#.to(DEVICE)\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e69e5f7b-8740-4d96-8d0b-3a95ca980584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    x,y=pred.predictions, pred.label_ids\n",
    "    return {\"MSE\":sklearn.metrics.mean_squared_error(x, y),\n",
    "           \"MAE\":sklearn.metrics.mean_absolute_error(x,y),\n",
    "           \"R2\":sklearn.metrics.r2_score(x,y)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1989bc74-fc6d-40fe-a767-fcab6badc693",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    trainer_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "79dfd13b-5a0f-48d7-a6cc-f80093c1c6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8092214465141296,\n",
       " 'eval_model_preparation_time': 0.0124,\n",
       " 'eval_MSE': 0.8092216849327087,\n",
       " 'eval_MAE': 0.7503647804260254,\n",
       " 'eval_R2': -3.5388848781585693,\n",
       " 'eval_runtime': 17.8937,\n",
       " 'eval_samples_per_second': 105.68,\n",
       " 'eval_steps_per_second': 3.353}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d71e818-0554-4738-9744-034534c894b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

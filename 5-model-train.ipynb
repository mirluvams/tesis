{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b858227-0d3a-4502-a716-bf4260d52c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model_names=[\n",
    "    \"microsoft/swinv2-tiny-patch4-window16-256\",\n",
    "    \"facebook/dinov2-base\",\n",
    "    \"nvidia/MambaVision-B-1K\",\n",
    "    \"microsoft/beit-base-patch16-224\",\n",
    "    \"microsoft/swinv2-base-patch4-window16-256\",\n",
    "    \"google/vit-base-patch16-384\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db1d24c-7e00-4c33-be92-f5a9e484904b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08032e1e-4411-456b-a17e-6d43d67b72dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eec179d8-dc2d-4c7c-849a-47ea0ee249e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageMultiRegressionModel(nn.Module):\n",
    "    def __init__(self, model, loss=nn.MSELoss(), output_size=1):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.classifier = nn.Linear(self.model.config.hidden_size, output_size)\n",
    "        self.loss=loss\n",
    "    \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        outputs = self.model(pixel_values=pixel_values)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # image embedding\n",
    "        values = self.classifier(cls_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss(values.view(-1), labels.view(-1))\n",
    "        return (loss, values) if loss is not None else values\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "396a8295-7ba5-402b-b317-c67aca19250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "951548d2-2e94-40f4-b928-ee8c20ad55b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3860a5e5-304f-4786-86c9-400d7c90d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model=AutoModel.from_pretrained(model_names[0])\n",
    "wrapped_model=ImageMultiRegressionModel(raw_model, output_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f312b03f-8861-48d2-b8a1-969b7f2571fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor=AutoImageProcessor.from_pretrained(model_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "242735e2-24cb-4e00-b8a3-12b35e138857",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=image_processor(Image.open(\"./images/2024/image_2.jpg\"), return_tensors=\"pt\")[\"pixel_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6007b35-6ff3-43d1-8675-06d54a9701c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=wrapped_model(inputs, labels=torch.tensor([[0,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7a65049-3f88-4eb0-a3c6-ef9a8d179d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0099, grad_fn=<MseLossBackward0>),\n",
       " tensor([[ 0.1490, -0.0094,  0.0852]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75887b88-fbac-40a4-ad75-aacb008ed968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee784ab-95e3-4633-b3a7-9730922c8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NCCL_P2P_DISABLE\"]=\"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"]=\"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6fafc44-de2e-4b7e-bdc6-586c10c9f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_args = TrainingArguments(\n",
    "    f\"~/tmp/model-training/modelname\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    #learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=12,\n",
    "    #gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=25,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=25,\n",
    "    load_best_model_at_end=True,\n",
    "    #metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    "    report_to=[]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c4c7c9d-75fa-41a4-8d8f-c7252961d5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset=load_from_disk(\"./data/dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a56308e0-efba-4b15-a407-72c4b41097c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c26803c3fe46ad839eb1313550242d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "dataset[\"test\"] = dataset[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08933ca7-f840-4d88-98d9-dbdee528fa1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b06f2fc3b84461a86f87884f8cb8f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7ff420505510>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/miriam/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7ff420505510>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/miriam/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import torchvision.transforms as transforms\n",
    "transform = transforms.Compose([transforms.PILToTensor()])\n",
    "\n",
    "def totensor(ex):\n",
    "    ex[\"pixel_values\"]=transform(ex[\"pixel_values\"])\n",
    "    return ex\n",
    "dataset[\"train\"]=dataset[\"train\"].map(totensor, num_proc=1)\n",
    "dataset[\"test\"]=dataset[\"test\"].map(totensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e0d7b-e0e1-4d73-815a-d8bee01a9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0][\"pixel_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1989bc74-fc6d-40fe-a767-fcab6badc693",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    wrapped_model,\n",
    "    trainer_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=image_processor,\n",
    "    #compute_metrics=compute_metrics,\n",
    "    #data_collator=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dfd13b-5a0f-48d7-a6cc-f80093c1c6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999343e3-3880-42e4-bbb8-b08fd7e0ee5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
